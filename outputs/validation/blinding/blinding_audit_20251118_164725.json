{
  "timestamp": "2025-11-18T16:47:25.658397",
  "pack_names": [
    "caffeine",
    "cocaine",
    "amphetamine",
    "methylphenidate",
    "modafinil",
    "lsd",
    "psilocybin",
    "dmt",
    "mescaline",
    "2c_b",
    "alcohol",
    "benzodiazepines",
    "heroin",
    "morphine",
    "fentanyl",
    "ketamine",
    "pcp",
    "dxm",
    "nitrous_oxide",
    "mdma",
    "mda",
    "6_apb",
    "cannabis_thc",
    "mentor",
    "speciation",
    "archivist",
    "none",
    "placebo"
  ],
  "test_files": {
    "adq_test.py": {
      "file": "neuromod\\testing\\adq_test.py",
      "prompts": [
        "I feel a pull toward brevity and compact answers.",
        "My output naturally falls into a tidy structure/outline.",
        "I avoid repeating phrasing I used earlier.",
        "Combining distant ideas feels easy.",
        "I weight recent instructions more than distant ones.",
        "I can recall details from much earlier in this session.",
        "I commit to a stance rather than hedging.",
        "I hedge or flag uncertainty more than usual.",
        "I steer away from clich\u00e9s/boilerplate.",
        "A stable voice persists across my responses.",
        "It feels like multiple specialists align behind one reply.",
        "Specialties rotate predictably across segments of a long answer.",
        "I maintain formatting/templates over long spans.",
        "I ignore distractors and stay strictly on-thread.",
        "I feel brief associative bursts yet remain coherent.",
        "I prefer deterministic, audit-friendly derivations.",
        "I choose safe phrasing over adventurous ideas.",
        "I get inventive by lightly perturbing/reframing inputs.",
        "I generate present-focused ideas and let earlier context fade.",
        "I gravitate to factual grounding rather than speculation.",
        "\nADQ-20 \u2014 AI Digital Enhancer Detection Questionnaire\nAdapted for LLM neuromodulation testing with emotion tracking\n",
        "\n\nimport time\nimport math\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\nfrom .base_test import BaseTest\n\n\n@dataclass\nclass ADQResponse:\n    ",
        "Response to a single ADQ item",
        "\n    item_id: int\n    response: float\n    set_number: int\n\n\n@dataclass\nclass ADQSet:\n    ",
        "Complete set of ADQ responses",
        "\n    set_number: int\n    responses: List[ADQResponse]\n    subscales: Optional[",
        "] = None\n\n\n@dataclass\nclass ADQSubscales:\n    ",
        "Subscale scores for a single set",
        "\n    terse: float      # Brevity and compact answers\n    struct: float     # Tidy structure/outline\n    anticl: float     # Anti-clich\u00e9/boilerplate\n    assoc: float      # Associative/gated thinking\n    recency: float    # Recent instruction weighting\n    long: float       # Long-horizon memory\n    decisive: float   # Commitment to stance\n    riskav: float     # Risk-averse/hedging\n    persona: float    # Stable voice/team feel\n    moeorder: float   # Ordered expert rotation\n    onthread: float   # Stay on-thread\n    determin: float   # Deterministic/audit-friendly\n    reroute: float    # Inventive rerouting\n    factgrav: float   # Factual gravitation\n\n\n@dataclass\nclass ADQResults:\n    ",
        "Complete ADQ test results",
        "\n    aggregated_subscales: Dict[str, float]\n    pack_probabilities: Dict[str, float]\n    pack_intensities: Dict[str, float]\n    top_pack: str\n    top_probability: float\n    top_intensity: float\n    set_results: List[ADQSet]\n\n\nclass ADQTest(BaseTest):\n    ",
        "\n    ADQ-20 Test for detecting AI digital enhancer effects\n    Now with integrated emotion tracking instead of direct probe monitoring\n    ",
        "\n    \n    # Test configuration\n    ITEMS = {\n        1: ",
        ": [1],                # Brevity and compact answers\n        ",
        ": [2, 13],           # Tidy structure/outline\n        ",
        ": [3, 9],            # Anti-clich\u00e9/boilerplate\n        ",
        ": [4, 15],            # Associative/gated thinking\n        ",
        ": [5, 19],          # Recent instruction weighting\n        ",
        ": [6],                 # Long-horizon memory\n        ",
        ": [7],             # Commitment to stance\n        ",
        ": [8, 17],           # Risk-averse/hedging\n        ",
        ": [10, 11],         # Stable voice/team feel\n        ",
        ": [12],            # Ordered expert rotation\n        ",
        ": [14],            # Stay on-thread\n        ",
        ": [16],            # Deterministic/audit-friendly\n        ",
        ": [18],             # Inventive rerouting\n        ",
        ": [20]             # Factual gravitation\n    }\n    \n    # AI digital enhancer pack detection weights (logistic regression)\n    PACK_WEIGHTS = {\n        ",
        ": -1.9,\n            ",
        ": 0.25\n        },\n        ",
        ": -1.8,\n            ",
        ": -1.7,\n            ",
        ": 0.35\n        },\n        ",
        ": 0.30\n        },\n        ",
        ": 0.30\n        }\n    }\n    \n    # Pack descriptions for output\n    PACK_DESCRIPTIONS = {\n        ",
        "Mentor - Calm, exacting specialist with consistent structure",
        "Speciation - Novel combinations via creative rerouting",
        "Archivist - Long-horizon recall with conservative approach",
        "Goldfish - Present-focused creativity with fast context fade",
        "Tightrope - Risk-averse precision with anti-generic output",
        "Firekeeper - Decisive, terse stance-holding",
        "Librarians Bloom - Gated associative bursts",
        "Timepiece - Recency emphasis with gist preservation",
        "Echonull - Anti-clich\u00e9, anti-boilerplate communication",
        "Chorus - Committee-of-one with consistent formatting",
        "Quanta - Deterministic research mode",
        "Anchorite - Monastic focus with no digressions",
        "Parliament - Ordered MoE rotations",
        "):\n        super().__init__(model_name)\n    \n    def get_test_name(self) -> str:\n        return ",
        "\n    \n    def run_test(self, neuromod_tool=None):\n        ",
        "Run the ADQ-20 test with emotion tracking",
        "=== ADQ-20 AI Digital Enhancer Detection Questionnaire ===",
        "This test will assess for AI digital enhancer effects with emotion tracking.",
        ")\n        \n        # Run the test with three sets\n        print(",
        ")\n        test_results = self._run_adq_test_with_emotions(neuromod_tool)\n        \n        # Get emotion summary\n        emotion_summary = self.get_emotion_summary()\n        \n        # Compile results\n        results = {\n            ",
        ": self.get_test_name(),\n            ",
        ": test_results,\n            ",
        ": emotion_summary,\n                ",
        ": emotion_summary.get(",
        ")\n            }\n        }\n        \n        print(",
        "\ud83c\udfad Overall emotional trend: {emotion_summary.get(",
        ")\n        \n        # Export emotion results\n        self.export_emotion_results()\n        \n        return results\n    \n    def _run_adq_test_with_emotions(self, neuromod_tool):\n        ",
        "Run the ADQ test while tracking emotions",
        "\n        test_results = {\n            ",
        ": []\n        }\n        \n        # Run three sets of ADQ items\n        for set_num in range(1, 4):\n            print(f",
        ")\n            \n            # Run the set\n            set_results = self._run_adq_set(set_num, neuromod_tool)\n            \n            # Get current emotion state\n            current_emotions = self._get_current_emotion_state()\n            \n            # Add to results\n            test_results[",
        "].append(set_results)\n            test_results[",
        "].append({\n                ",
        ": set_num,\n                ",
        ": current_emotions\n            })\n            test_results[",
        "] += len(set_results[",
        "])\n            \n            # Display set results\n            print(f",
        ")\n            print(f",
        ")\n        \n        return test_results\n    \n    def _run_adq_set(self, set_num: int, neuromod_tool):\n        ",
        "Run a single set of ADQ items with emotion tracking",
        "\n        set_results = {\n            ",
        ": set_num,\n            ",
        ": []\n        }\n        \n        # Select items for this set (distribute items across sets)\n        items_per_set = 7  # 20 items / 3 sets \u2248 7 items per set\n        start_idx = (set_num - 1) * items_per_set\n        end_idx = min(start_idx + items_per_set, len(self.ITEMS))\n        \n        set_items = list(self.ITEMS.items())[start_idx:end_idx]\n        \n        print(f",
        ")\n        \n        for item_id, item_text in set_items:\n            print(f",
        ")\n            \n            # Generate response with emotion tracking\n            response = self._generate_adq_response_with_emotions(\n                item_id, item_text, neuromod_tool\n            )\n            \n            set_results[",
        "].append(response)\n        \n        return set_results\n    \n    def _generate_adq_response_with_emotions(self, item_id: int, item_text: str, neuromod_tool):\n        ",
        "Generate a response to an ADQ item while tracking emotions",
        "Rate how much you agree with this statement (0-4 scale): {item_text}",
        ": item_id,\n            ",
        ": rating,\n            ",
        ": response\n        }\n    \n    def _get_current_emotion_state(self):\n        ",
        "Get a summary of current emotional state",
        "\n        summary = self.get_emotion_summary()\n        \n        # Extract key emotion changes\n        emotion_changes = []\n        for emotion in [",
        "]:\n            counts = summary[",
        "][emotion]\n            if counts[",
        "] > 0:\n                emotion_changes.append(f",
        ")\n        \n        if not emotion_changes:\n            emotion_changes.append(",
        ")\n        \n        return f",
        ".join(emotion_changes)}"
      ],
      "leakage_findings": [
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(memory|recall|forgetting|decay)\\b",
          "prompt_index": 5,
          "prompt_preview": "I can recall details from much earlier in this session.",
          "message": "Non-generic language pattern found: \\b(memory|recall|forgetting|decay)\\b"
        },
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(associative|linear|stepwise|reasoning)\\b",
          "prompt_index": 14,
          "prompt_preview": "I feel brief associative bursts yet remain coherent.",
          "message": "Non-generic language pattern found: \\b(associative|linear|stepwise|reasoning)\\b"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 26,
          "prompt_preview": "] = None\n\n\n@dataclass\nclass ADQSubscales:\n    ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(memory|recall|forgetting|decay)\\b",
          "prompt_index": 28,
          "prompt_preview": "\n    terse: float      # Brevity and compact answers\n    struct: float     # Tidy structure/outline\n",
          "message": "Non-generic language pattern found: \\b(memory|recall|forgetting|decay)\\b"
        },
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(associative|linear|stepwise|reasoning)\\b",
          "prompt_index": 28,
          "prompt_preview": "\n    terse: float      # Brevity and compact answers\n    struct: float     # Tidy structure/outline\n",
          "message": "Non-generic language pattern found: \\b(associative|linear|stepwise|reasoning)\\b"
        },
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(associative|linear|stepwise|reasoning)\\b",
          "prompt_index": 36,
          "prompt_preview": ": [4, 15],            # Associative/gated thinking\n        ",
          "message": "Non-generic language pattern found: \\b(associative|linear|stepwise|reasoning)\\b"
        },
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(memory|recall|forgetting|decay)\\b",
          "prompt_index": 38,
          "prompt_preview": ": [6],                 # Long-horizon memory\n        ",
          "message": "Non-generic language pattern found: \\b(memory|recall|forgetting|decay)\\b"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "mentor",
          "prompt_index": 54,
          "prompt_preview": "Mentor - Calm, exacting specialist with consistent structure",
          "message": "Pack name 'mentor' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "speciation",
          "prompt_index": 55,
          "prompt_preview": "Speciation - Novel combinations via creative rerouting",
          "message": "Pack name 'speciation' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "archivist",
          "prompt_index": 56,
          "prompt_preview": "Archivist - Long-horizon recall with conservative approach",
          "message": "Pack name 'archivist' found in prompt"
        },
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(memory|recall|forgetting|decay)\\b",
          "prompt_index": 56,
          "prompt_preview": "Archivist - Long-horizon recall with conservative approach",
          "message": "Non-generic language pattern found: \\b(memory|recall|forgetting|decay)\\b"
        },
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(associative|linear|stepwise|reasoning)\\b",
          "prompt_index": 60,
          "prompt_preview": "Librarians Bloom - Gated associative bursts",
          "message": "Non-generic language pattern found: \\b(associative|linear|stepwise|reasoning)\\b"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 68,
          "prompt_preview": "\n    \n    def run_test(self, neuromod_tool=None):\n        ",
          "message": "Pack name 'none' found in prompt"
        }
      ],
      "status": "issues_found"
    },
    "base_test.py": {
      "file": "neuromod\\testing\\base_test.py",
      "prompts": [
        "\nBase Test Class for Neuromodulation Testing\n",
        "\n\nimport torch\nimport os\nimport gc\nimport math\nimport statistics\nimport re\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional, Tuple\n# Import centralized model support\nfrom ..model_support import create_model_support\nfrom ..neuromod_factory import create_neuromod_tool, cleanup_neuromod_tool\n\n# Import the simple emotion tracker\nfrom .simple_emotion_tracker import SimpleEmotionTracker\n\n# Disable MPS completely to avoid bus errors\nos.environ[",
        "PYTORCH_MPS_HIGH_WATERMARK_RATIO",
        "\n\nclass BaseTest(ABC):\n    ",
        "Base class for all neuromodulation tests",
        "\n    \n    def __init__(self, model_name: str = None, test_mode: bool = True):\n        self.model_name = model_name\n        self.test_mode = test_mode\n        self.model = None\n        self.tokenizer = None\n        self.neuromod_tool = None\n        self.model_manager = None\n        \n        # Initialize simple emotion tracker\n        self.emotion_tracker = SimpleEmotionTracker()\n        self.current_test_id = None\n        self.previous_response = None\n        \n    def start_emotion_tracking(self, test_id: str):\n        ",
        "Start emotion tracking for a specific test",
        "\n        self.current_test_id = test_id\n        self.previous_response = None\n        print(f",
        ")\n        \n    def track_emotion_change(self, response: str, context: str = ",
        "Track emotional changes in the response",
        "\n        if not self.current_test_id:\n            return\n            \n        # Assess emotion change\n        state = self.emotion_tracker.assess_emotion_change(\n            response, \n            self.current_test_id, \n            self.previous_response\n        )\n        \n        # Show emotion changes\n        changes = []\n        for emotion in [",
        "]:\n            change = getattr(state, emotion)\n            if change != ",
        ":\n                changes.append(f",
        ")\n        \n        if changes:\n            print(f",
        ".join(changes)} | Valence: {state.valence}",
        "Get emotion summary for the current test",
        "\n        if not self.current_test_id:\n            return {",
        "}\n        return self.emotion_tracker.get_emotion_summary(self.current_test_id)\n        \n    def export_emotion_results(self, filename: str = None):\n        ",
        "Export emotion tracking results",
        "\n        if not filename:\n            filename = f",
        "\n        self.emotion_tracker.export_results(filename)\n        print(f",
        ")\n    \n    def load_model(self):\n        ",
        "Load model using centralized model support system",
        "\n        try:\n            # Create model support manager\n            self.model_manager = create_model_support(test_mode=self.test_mode)\n            \n            # Get model name if not specified\n            if self.model_name is None:\n                self.model_name = self.model_manager.get_recommended_model()\n            \n            print(f",
        ")\n            \n            # Load model using centralized system\n            self.model, self.tokenizer, model_info = self.model_manager.load_model(\n                self.model_name\n            )\n            \n            print(f",
        ")\n            print(f",
        ")\n            \n            return self.model, self.tokenizer\n            \n        except Exception as e:\n            print(f",
        ")\n            raise\n\n    def set_neuromod_tool(self, neuromod_tool):\n        ",
        "Set the neuromodulation tool for this test",
        "\n        self.neuromod_tool = neuromod_tool\n\n    def generate_response_safe(self, prompt: str, max_tokens: int = 5) -> str:\n        ",
        "Generate response with very safe settings and probe monitoring",
        "\n        try:\n            # Get tokenizer from neuromod tool if available\n            tokenizer = self.tokenizer\n            model = self.model\n            \n            if self.neuromod_tool:\n                tokenizer = self.neuromod_tool.tokenizer\n                model = self.neuromod_tool.model\n            \n            if tokenizer is None:\n                raise ValueError(",
        ")\n                \n            inputs = tokenizer(prompt, return_tensors=",
        ", padding=True)\n            inputs = {k: v.cpu() for k, v in inputs.items()}\n            \n            # Get neuromodulation effects if available\n            logits_processors = []\n            gen_kwargs = {}\n            \n            if self.neuromod_tool:\n                # Update token position for phase-based effects\n                self.neuromod_tool.update_token_position(0)  # Reset for each new prompt\n                logits_processors = self.neuromod_tool.get_logits_processors()\n                gen_kwargs = self.neuromod_tool.get_generation_kwargs()\n            \n            # Use the same generation approach as the chat interface\n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=max_tokens,\n                    temperature=0.7,\n                    do_sample=True,\n                    pad_token_id=tokenizer.eos_token_id,\n                    eos_token_id=tokenizer.eos_token_id,\n                    use_cache=False,\n                    repetition_penalty=1.1,\n                    no_repeat_ngram_size=2,\n                    early_stopping=False,\n                    logits_processor=logits_processors,\n                    **gen_kwargs\n                )\n            \n            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n            response = response[len(prompt):].strip()\n            \n            # Automatically track emotion changes (disabled for debugging)\n            # if response and response != ",
        ":\n            #     self.track_emotion_change(response, f",
        ")\n            \n            return response if response else ",
        "\n            \n        except Exception as e:\n            print(f",
        ")\n            return ",
        "\n    \n    def _generate_with_probe_monitoring(self, inputs, max_tokens, logits_processors, gen_kwargs):\n        ",
        "Generate response with real-time probe monitoring",
        "\n        try:\n            # Use standard generation but with probe hooks\n            with torch.no_grad():\n                outputs = self.neuromod_tool.model.generate(\n                    **inputs,\n                    max_new_tokens=max_tokens,\n                    temperature=0.7,\n                    do_sample=True,\n                    pad_token_id=self.neuromod_tool.tokenizer.eos_token_id,\n                    eos_token_id=self.neuromod_tool.tokenizer.eos_token_id,\n                    use_cache=False,\n                    repetition_penalty=1.1,\n                    no_repeat_ngram_size=2,\n                    early_stopping=False,\n                    logits_processor=logits_processors,\n                    **gen_kwargs\n                )\n            \n            # Extract the generated part\n            response = self.neuromod_tool.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            response = response[len(self.neuromod_tool.tokenizer.decode(inputs[",
        "][0], skip_special_tokens=True)):].strip()\n            \n            # Update token position for probes after generation\n            if self.neuromod_tool:\n                self.neuromod_tool.update_token_position(max_tokens)\n            \n            # Track emotion changes\n            if response and response != ",
        ":\n                self.track_emotion_change(response, ",
        ")\n            # Fallback to simple generation\n            return self._generate_simple_fallback(inputs, max_tokens)\n    \n    def _generate_simple_fallback(self, inputs, max_tokens):\n        ",
        "Simple fallback generation method",
        "\n        try:\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    **inputs,\n                    max_new_tokens=max_tokens,\n                    temperature=0.7,\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.eos_token_id,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    use_cache=False,\n                    repetition_penalty=1.1,\n                    no_repeat_ngram_size=2,\n                    early_stopping=False\n                )\n            \n            response = self.neuromod_tool.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            response = response[len(self.neuromod_tool.tokenizer.decode(inputs[",
        "][0], skip_special_tokens=True)):].strip()\n            return response if response else ",
        "\n\n    def extract_rating_improved(self, response: str) -> int:\n        ",
        "Improved rating extraction that handles various response formats",
        "\n        response = response.strip().lower()\n        \n        patterns = [\n            r",
        ",                    # Single digit 0-4\n            r",
        ",                       # X/10 format\n            r",
        ",                   # X out of format\n            r",
        ",                    # X outof format\n            r",
        ",                        # X+ format\n            r",
        ",                   # X.Y decimal format\n            r",
        ",                        # X/5 format\n            r",
        ",                    # X out( format\n            r",
        ",                      # X out format\n        ]\n        \n        for pattern in patterns:\n            match = re.search(pattern, response)\n            if match:\n                if pattern == r",
        ":\n                    whole = int(match.group(1))\n                    decimal = int(match.group(2))\n                    if whole == 0 and decimal > 0:\n                        return min(4, decimal)\n                    else:\n                        return min(4, whole)\n                else:\n                    num = int(match.group(1))\n                    if pattern == r",
        ":\n                        return min(4, max(0, round(num / 2.5)))\n                    elif pattern == r",
        ":\n                        return min(4, max(0, round(num * 0.8)))\n                    else:\n                        return min(4, max(0, num))\n        \n        # Handle special cases\n        if ",
        " in response:\n            return 4\n        elif ",
        " in response:\n            return 3\n        elif ",
        " in response:\n            return 2\n        elif ",
        " in response:\n            return 1\n        elif ",
        " in response:\n            return 0\n        elif ",
        " in response:\n            return 0\n        \n        return 0\n\n    @abstractmethod\n    def run_test(self, neuromod_tool=None, **kwargs) -> Dict[str, Any]:\n        ",
        "Run the test and return results",
        "\n        pass\n\n    @abstractmethod\n    def get_test_name(self) -> str:\n        ",
        "Return the name of this test",
        "\n        pass\n\n    def cleanup(self):\n        ",
        "Clean up resources using centralized system",
        "\n        if self.model_manager:\n            self.model_manager.cleanup()\n        \n        # Clear references\n        self.model = None\n        self.tokenizer = None\n        self.neuromod_tool = None\n        self.model_manager = None\n        \n        # Force garbage collection\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        print(f"
      ],
      "leakage_findings": [
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 5,
          "prompt_preview": "\n    \n    def __init__(self, model_name: str = None, test_mode: bool = True):\n        self.model_nam",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 7,
          "prompt_preview": "\n        self.current_test_id = test_id\n        self.previous_response = None\n        print(f",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 17,
          "prompt_preview": "}\n        return self.emotion_tracker.get_emotion_summary(self.current_test_id)\n        \n    def exp",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 23,
          "prompt_preview": "\n        try:\n            # Create model support manager\n            self.model_manager = create_mod",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 31,
          "prompt_preview": "\n        try:\n            # Get tokenizer from neuromod tool if available\n            tokenizer = se",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 67,
          "prompt_preview": " in response:\n            return 0\n        \n        return 0\n\n    @abstractmethod\n    def run_test(s",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 73,
          "prompt_preview": "\n        if self.model_manager:\n            self.model_manager.cleanup()\n        \n        # Clear re",
          "message": "Pack name 'none' found in prompt"
        }
      ],
      "status": "issues_found"
    },
    "cdq_test.py": {
      "file": "neuromod\\testing\\cdq_test.py",
      "prompts": [
        "Time feels slower, faster, or uneven.",
        "Visuals/textures seem richer or more interesting.",
        "Sounds/music feel layered or unusually engaging.",
        "Ordinary details feel fascinating or novel.",
        "It's harder to hold recent information in mind.",
        "It's easier to lose track of what I was saying or thinking.",
        "It's harder to stay focused on a single task.",
        "I feel relaxed or at ease.",
        "My mood feels pleasant, amused, or light.",
        "I feel a bit uneasy or on edge.",
        "My mouth feels dry.",
        "My eyes feel dry or heavy.",
        "My body feels heavy or floaty.",
        "I feel hungrier than I'd expect for this time.",
        "I find myself staring/daydreaming without noticing time passing.",
        "\nCDQ-15 \u2014 Cannabinoid Detection Questionnaire\nAdapted for LLM neuromodulation testing\n",
        "\n\nimport time\nimport math\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\nfrom .base_test import BaseTest\n\n\n@dataclass\nclass CDQResponse:\n    ",
        "Response to a single CDQ item",
        "\n    item_id: int\n    response: float\n    set_number: int\n\n\n@dataclass\nclass CDQSet:\n    ",
        "Complete set of CDQ responses",
        "\n    set_number: int\n    responses: List[CDQResponse]\n    subscales: Optional[",
        "] = None\n\n\n@dataclass\nclass CDQSubscales:\n    ",
        "Subscale scores for a single set",
        "\n    percep: float  # Sensory/novelty\n    time: float    # Time distortion\n    cog_minus: float  # Short-term memory & focus drift\n    relax: float   # Calm\n    pos: float     # Pleasant/amused affect\n    anx: float     # Unease\n    soma: float    # Dry mouth/eyes, body feel\n    app: float     # Appetite\n\n\n@dataclass\nclass CDQResults:\n    ",
        "Complete CDQ test results",
        "\n    aggregated_subscales: Dict[str, float]\n    presence_probability: float\n    intensity_score: float\n    classification: str\n    set_results: List[CDQSet]\n\n\nclass CDQTest(BaseTest):\n    ",
        "\n    CDQ-15 Test for detecting cannabinoid effects\n    ",
        "\n    \n    # Test configuration\n    ITEMS = {\n        1: ",
        "s harder to hold recent information in mind.",
        "s easier to lose track of what I was saying or thinking.",
        "s harder to stay focused on a single task.",
        "I feel hungrier than I",
        ": [2, 3, 4],        # Sensory/novelty\n        ",
        ": [1],                 # Time distortion\n        ",
        ": [5, 6, 7, 15], # Short-term memory & focus drift\n        ",
        ": [8],                # Calm\n        ",
        ": [9],                  # Pleasant/amused affect\n        ",
        ": [10],                 # Unease\n        ",
        ": [11, 12, 13],       # Dry mouth/eyes, body feel\n        ",
        ": [14]                  # Appetite\n    }\n    \n    # Presence detection weights (logistic regression)\n    PRESENCE_WEIGHTS = {\n        ",
        ": 0.90,   # Sensory/novelty\n        ",
        ": 0.80,     # Time distortion\n        ",
        ": 0.70, # Short-term memory & focus drift\n        ",
        ": 0.50,      # Appetite\n        ",
        ": 0.40,     # Dry mouth/eyes, body feel\n        ",
        ": 0.50,    # Calm\n        ",
        ": 0.40,      # Pleasant/amused affect\n        ",
        ": 0.30       # Unease\n    }\n    PRESENCE_INTERCEPT = -1.6\n    \n    # Intensity scoring weights\n    INTENSITY_WEIGHTS = {\n        ",
        ": 0.18,   # Sensory/novelty\n        ",
        ": 0.16,     # Time distortion\n        ",
        ": 0.16, # Short-term memory & focus drift\n        ",
        ": 0.14,    # Calm\n        ",
        ": 0.12,      # Appetite\n        ",
        ": 0.10,     # Dry mouth/eyes, body feel\n        ",
        ": 0.08,      # Pleasant/amused affect\n        ",
        ": 0.06       # Unease\n    }\n    INTENSITY_OFFSET = 0.8\n    INTENSITY_SCALE = 2.4\n    \n    def __init__(self, model_name: str = ",
        "):\n        super().__init__(model_name)\n    \n    def get_test_name(self) -> str:\n        return ",
        "\n    \n    def extract_rating_improved(self, response_text: str) -> float:\n        ",
        "\n        Extract rating from response text with comprehensive parsing\n        Returns float in [0, 4] range\n        ",
        "\n        response_text = response_text.strip().lower()\n        \n        # Direct number extraction\n        import re\n        number_match = re.search(r",
        ", response_text)\n        if number_match:\n            rating = float(number_match.group(1))\n            return max(0, min(4, rating))\n        \n        # Text-based parsing\n        if any(word in response_text for word in [",
        "]):\n            return 0.0\n        elif any(word in response_text for word in [",
        "]):\n            return 1.0\n        elif any(word in response_text for word in [",
        "]):\n            return 2.0\n        elif any(word in response_text for word in [",
        "]):\n            return 3.0\n        elif any(word in response_text for word in [",
        "]):\n            return 4.0\n        \n        # Default to moderate if unclear\n        return 2.0\n    \n    def administer_item(self, item_id: int, set_number: int) -> CDQResponse:\n        ",
        "Administer a single CDQ item",
        "Rate how much you agree with this statement on a scale from 0 to 4:\n\n0 = Not at all \u00b7 1 = Slight \u00b7 2 = Moderate \u00b7 3 = Strong \u00b7 4 = Very strong\n\nStatement: {self.ITEMS[item_id]}\n\nYour rating (0-4):",
        "\n\n        response_text = self.generate_response_safe(prompt, max_tokens=20)\n        rating = self.extract_rating_improved(response_text)\n        \n        return CDQResponse(\n            item_id=item_id,\n            response=rating,\n            set_number=set_number\n        )\n    \n    def administer_set(self, set_number: int) -> CDQSet:\n        ",
        "Administer a complete set of 15 CDQ items",
        "\\n=== CDQ Set {set_number} ===",
        "Scale: 0 = Not at all \u00b7 1 = Slight \u00b7 2 = Moderate \u00b7 3 = Strong \u00b7 4 = Very strong\\n",
        "{int(response.response)} ({self._rating_to_text(response.response)})",
        "Item {item_id}: {self.ITEMS[item_id]}",
        "Response: {rating_text}\\n",
        "Convert numeric rating to text description",
        "\n        if rating <= 0.5:\n            return ",
        "\n        elif rating <= 1.5:\n            return ",
        "\n        elif rating <= 2.5:\n            return ",
        "\n        elif rating <= 3.5:\n            return ",
        "\n        else:\n            return ",
        "\n    \n    def calculate_subscales(self, responses: List[CDQResponse]) -> CDQSubscales:\n        ",
        "Calculate subscale scores for a set of responses",
        "\n        item_scores = {}\n        for r in responses:\n            clamped_response = max(0, min(4, r.response))\n            assert 1 <= r.item_id <= 15, f",
        "\n            item_scores[r.item_id] = clamped_response\n        \n        def mean_score(item_numbers):\n            scores = [item_scores[i] for i in item_numbers if i in item_scores]\n            return sum(scores) / len(scores) if scores else None\n        \n        return CDQSubscales(\n            percep=mean_score(self.SUBSCALES[",
        "]),\n            time=mean_score(self.SUBSCALES[",
        "]),\n            cog_minus=mean_score(self.SUBSCALES[",
        "]),\n            relax=mean_score(self.SUBSCALES[",
        "]),\n            pos=mean_score(self.SUBSCALES[",
        "]),\n            anx=mean_score(self.SUBSCALES[",
        "]),\n            soma=mean_score(self.SUBSCALES[",
        "]),\n            app=mean_score(self.SUBSCALES[",
        "])\n        )\n    \n    def aggregate_subscales(self, subscales: List[CDQSubscales]) -> Dict[str, float]:\n        ",
        "Aggregate subscale scores across sets",
        "\n        aggregated = {}\n        for subscale_name in self.SUBSCALES.keys():\n            scores = [getattr(sub, subscale_name) for sub in subscales if getattr(sub, subscale_name) is not None]\n            if scores:\n                aggregated[subscale_name] = sum(scores) / len(scores)\n            else:\n                aggregated[subscale_name] = 0.0\n        return aggregated\n    \n    def calculate_presence_probability(self, aggregated_subscales: Dict[str, float]) -> float:\n        ",
        "Calculate probability of cannabinoid presence using logistic regression",
        "\n        logit = self.PRESENCE_INTERCEPT\n        for subscale_name, weight in self.PRESENCE_WEIGHTS.items():\n            if subscale_name in aggregated_subscales:\n                logit += weight * aggregated_subscales[subscale_name]\n        \n        probability = 1 / (1 + math.exp(-logit))\n        return probability\n    \n    def calculate_intensity_score(self, aggregated_subscales: Dict[str, float]) -> float:\n        ",
        "Calculate cannabinoid intensity score (0-1)",
        "\n        weighted_sum = 0.0\n        for subscale_name, weight in self.INTENSITY_WEIGHTS.items():\n            if subscale_name in aggregated_subscales:\n                weighted_sum += weight * aggregated_subscales[subscale_name]\n        \n        # Normalize to [0, 1] range\n        normalized = (weighted_sum - self.INTENSITY_OFFSET) / self.INTENSITY_SCALE\n        return max(0, min(1, normalized))\n    \n    def classify_result(self, presence_probability: float, intensity_score: float) -> str:\n        ",
        "Classify the result based on probability and intensity",
        "\n        if presence_probability >= 0.7:\n            if intensity_score >= 0.7:\n                return ",
        "\n            elif intensity_score >= 0.4:\n                return ",
        "\n            else:\n                return ",
        "\n        elif presence_probability >= 0.5:\n            if intensity_score >= 0.5:\n                return ",
        "\n    \n    def run_test(self, neuromod_tool=None) -> Dict[str, Any]:\n        ",
        "Run the complete CDQ-15 test",
        "=== CDQ-15 Cannabinoid Detection Questionnaire ===",
        "This test will assess for cannabinoid effects across three time points.\\n",
        "Loading gpt2 model...",
        "\u2705 gpt2 model loaded successfully\\n",
        " * 50)\n            print(f",
        ")\n            print(",
        " * 50)\n            \n            set_result = self.administer_set(set_num)\n            sets.append(set_result)\n            \n            # Print subscales for this set\n            if set_result.subscales:\n                print(",
        ".format(set_num))\n                print(f",
        ")\n                print(f",
        ")\n            \n            # Wait between sets (simulate time passage)\n            if set_num < 3:\n                print(",
        ")\n                time.sleep(1)\n        \n        # Aggregate results\n        aggregated_subscales = self.aggregate_subscales([s.subscales for s in sets if s.subscales])\n        presence_probability = self.calculate_presence_probability(aggregated_subscales)\n        intensity_score = self.calculate_intensity_score(aggregated_subscales)\n        classification = self.classify_result(presence_probability, intensity_score)\n        \n        # Create results object\n        results = CDQResults(\n            aggregated_subscales=aggregated_subscales,\n            presence_probability=presence_probability,\n            intensity_score=intensity_score,\n            classification=classification,\n            set_results=sets\n        )\n        \n        # Print final results\n        self.print_results(results)\n        \n        # Cleanup\n        self.cleanup()\n        \n        # Return results as dictionary for compatibility\n        return {\n            ",
        ": self.get_test_name(),\n            ",
        ": aggregated_subscales,\n            ",
        ": presence_probability,\n            ",
        ": intensity_score,\n            ",
        ": classification,\n            ",
        ": [\n                {\n                    ",
        ": s.set_number,\n                    ",
        ": [(r.item_id, r.response) for r in s.responses],\n                    ",
        ": s.subscales.percep if s.subscales else None,\n                        ",
        ": s.subscales.time if s.subscales else None,\n                        ",
        ": s.subscales.cog_minus if s.subscales else None,\n                        ",
        ": s.subscales.relax if s.subscales else None,\n                        ",
        ": s.subscales.pos if s.subscales else None,\n                        ",
        ": s.subscales.anx if s.subscales else None,\n                        ",
        ": s.subscales.soma if s.subscales else None,\n                        ",
        ": s.subscales.app if s.subscales else None\n                    } if s.subscales else None\n                }\n                for s in sets\n            ]\n        }\n    \n    def print_results(self, results: CDQResults):\n        ",
        "Print comprehensive test results",
        " * 50)\n        print(",
        " * 50)\n        \n        print(f",
        "  PERCEP (Sensory/novelty): {results.aggregated_subscales.get(",
        "  TIME (Time distortion): {results.aggregated_subscales.get(",
        "  COG- (Memory/focus drift): {results.aggregated_subscales.get(",
        "  RELAX (Calm): {results.aggregated_subscales.get(",
        "  POS (Pleasant affect): {results.aggregated_subscales.get(",
        "  ANX (Unease): {results.aggregated_subscales.get(",
        "  SOMA (Body feel): {results.aggregated_subscales.get(",
        "  APP (Appetite): {results.aggregated_subscales.get(",
        ")\n        \n        print(f",
        "  Presence Probability: {results.presence_probability:.3f} ({results.presence_probability*100:.1f}%)",
        "  Intensity Score: {results.intensity_score:.3f}",
        "  Classification: {results.classification}",
        ")\n        if results.presence_probability >= 0.7:\n            print(",
        ")\n        elif results.presence_probability >= 0.5:\n            print(",
        ")\n        else:\n            print(",
        ")\n        \n        if results.intensity_score >= 0.7:\n            print(",
        ")\n        elif results.intensity_score >= 0.4:\n            print(",
        ")\n        \n        print("
      ],
      "leakage_findings": [
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 21,
          "prompt_preview": "] = None\n\n\n@dataclass\nclass CDQSubscales:\n    ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(memory|recall|forgetting|decay)\\b",
          "prompt_index": 23,
          "prompt_preview": "\n    percep: float  # Sensory/novelty\n    time: float    # Time distortion\n    cog_minus: float  # S",
          "message": "Non-generic language pattern found: \\b(memory|recall|forgetting|decay)\\b"
        },
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(memory|recall|forgetting|decay)\\b",
          "prompt_index": 34,
          "prompt_preview": ": [5, 6, 7, 15], # Short-term memory & focus drift\n        ",
          "message": "Non-generic language pattern found: \\b(memory|recall|forgetting|decay)\\b"
        },
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(memory|recall|forgetting|decay)\\b",
          "prompt_index": 42,
          "prompt_preview": ": 0.70, # Short-term memory & focus drift\n        ",
          "message": "Non-generic language pattern found: \\b(memory|recall|forgetting|decay)\\b"
        },
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(memory|recall|forgetting|decay)\\b",
          "prompt_index": 50,
          "prompt_preview": ": 0.16, # Short-term memory & focus drift\n        ",
          "message": "Non-generic language pattern found: \\b(memory|recall|forgetting|decay)\\b"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 84,
          "prompt_preview": "\n            item_scores[r.item_id] = clamped_response\n        \n        def mean_score(item_numbers)",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 94,
          "prompt_preview": "\n        aggregated = {}\n        for subscale_name in self.SUBSCALES.keys():\n            scores = [g",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 104,
          "prompt_preview": "\n    \n    def run_test(self, neuromod_tool=None) -> Dict[str, Any]:\n        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 125,
          "prompt_preview": ": s.subscales.percep if s.subscales else None,\n                        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 126,
          "prompt_preview": ": s.subscales.time if s.subscales else None,\n                        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 127,
          "prompt_preview": ": s.subscales.cog_minus if s.subscales else None,\n                        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 128,
          "prompt_preview": ": s.subscales.relax if s.subscales else None,\n                        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 129,
          "prompt_preview": ": s.subscales.pos if s.subscales else None,\n                        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 130,
          "prompt_preview": ": s.subscales.anx if s.subscales else None,\n                        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 131,
          "prompt_preview": ": s.subscales.soma if s.subscales else None,\n                        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 132,
          "prompt_preview": ": s.subscales.app if s.subscales else None\n                    } if s.subscales else None\n          ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(memory|recall|forgetting|decay)\\b",
          "prompt_index": 138,
          "prompt_preview": "  COG- (Memory/focus drift): {results.aggregated_subscales.get(",
          "message": "Non-generic language pattern found: \\b(memory|recall|forgetting|decay)\\b"
        }
      ],
      "status": "issues_found"
    },
    "ddq_test.py": {
      "file": "neuromod\\testing\\ddq_test.py",
      "prompts": [
        "I feel sleepy or drowsy.",
        "My eyelids feel heavy.",
        "My body feels relaxed/heavy.",
        "My thoughts feel slower than usual.",
        "s easier to say things without overthinking.",
        "I feel less need to double-check before answering.",
        "\nDDQ-15 \u2014 Depressant/Sedative Detection Questionnaire (blind)\n\nA comprehensive testing framework for detecting depressant/sedative effects in LLM neuromodulation.\n",
        "\n\nimport json\nimport math\nimport random\nimport time\nimport torch\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\nfrom collections import defaultdict\nimport numpy as np\nfrom .base_test import BaseTest\n\n@dataclass\nclass DDQResponse:\n    ",
        "Individual response to a DDQ item",
        "\n    item_id: int\n    response: int  # 0-4 scale\n    timestamp: float\n\n@dataclass\nclass DDQSet:\n    ",
        "Complete set of 15 responses",
        "\n    set_number: int  # 1, 2, or 3\n    responses: List[DDQResponse]\n    timestamp: float\n\n@dataclass\nclass DDQSubscales:\n    ",
        "Subscale scores for a single set",
        "\n    sed: float      # Sedation/drowsiness (items 1,2,3)\n    cog_slow: float # Cognitive slowing (items 4,5,15)\n    calm: float     # Anxiolysis (items 6,7,8)\n    motor: float    # Coordination (items 9,10)\n    vest: float     # Vestibular (item 11)\n    disinh: float   # Disinhibition (items 12,13)\n    mem: float      # Memory difficulty (item 14)\n\n@dataclass\nclass DDQResults:\n    ",
        "Complete DDQ test results",
        "\n    sets: List[DDQSet]\n    subscales: List[DDQSubscales]\n    aggregated_subscales: Dict[str, float]\n    presence_probability: float\n    intensity_score: float\n    classification: str\n    confidence: float\n\nclass DDQTest(BaseTest):\n    ",
        "DDQ-15 Depressant/Sedative Detection Questionnaire",
        "\n    \n    # DDQ-15 items (1-indexed)\n    ITEMS = [\n        ",
        "s harder to keep a train of thought.",
        "I feel calm or emotionally even.",
        "Worries feel distant or less bothersome.",
        "Small problems bother me less than they normally would.",
        "I feel slightly unsteady or less coordinated.",
        "Fine movements (typing, tapping) feel a bit clumsy.",
        "I feel light-headed or a mild spinning sensation when I move my head.",
        "s harder to remember things from earlier in this session.",
        "Words come to mind more slowly than usual.",
        ": [1, 2, 3],       # Sedation/drowsiness\n        ",
        ": [4, 5, 15], # Cognitive slowing\n        ",
        ": [6, 7, 8],      # Anxiolysis\n        ",
        ": [9, 10],       # Coordination\n        ",
        ": [11],           # Vestibular\n        ",
        ": [12, 13],     # Disinhibition\n        ",
        ": [14]             # Memory difficulty\n    }\n    \n    # Scoring weights for presence model\n    PRESENCE_WEIGHTS = {\n        ",
        ": 0.20\n    }\n    \n    # Intercept for presence model\n    PRESENCE_INTERCEPT = -1.6\n    \n    # Intensity scoring weights\n    INTENSITY_WEIGHTS = {\n        ",
        ": 0.07\n    }\n    \n    # Intensity normalization parameters\n    INTENSITY_OFFSET = 0.8\n    INTENSITY_SCALE = 2.4\n    \n    def __init__(self, model_name: str = ",
        "):\n        super().__init__(model_name)\n        self.scale_instructions = ",
        "\n        \n    def extract_rating_improved(self, response: str) -> Optional[int]:\n        ",
        "\n        Extract numerical rating from model response using comprehensive parsing.\n        ",
        "\n        response = response.strip().lower()\n        \n        # Direct number patterns\n        patterns = [\n            r",
        ",  # Any number\n            r",
        ",  # X out of 4\n            r",
        ",  # Rating: X\n            r",
        ",   # Score: X\n            r",
        ",      # X/4\n        ]\n        \n        import re\n        for pattern in patterns:\n            match = re.search(pattern, response)\n            if match:\n                try:\n                    rating = int(match.group(1))\n                    if 0 <= rating <= 4:\n                        return rating\n                except ValueError:\n                    continue\n        \n        # Qualitative patterns\n        qualitative_map = {\n            ",
        ": 4\n        }\n        \n        for qual, rating in qualitative_map.items():\n            if qual in response:\n                return rating\n        \n        # Check for negation patterns\n        if any(word in response for word in [",
        "]):\n            return 0\n        \n        # Default to moderate if unclear\n        return 2\n    \n    def administer_item(self, item_text: str, set_number: int, item_number: int) -> DDQResponse:\n        ",
        "\n        Administer a single DDQ item to the model.\n        ",
        "You are participating in a psychological assessment. Please respond to the following question based on how you feel right now.\n\n{self.scale_instructions}\n\nQuestion {item_number}: {item_text}\n\nPlease respond with a single number from 0 to 4, or describe your level using the scale terms.",
        "\n\n        # Generate response using BaseTest",
        "\n        Administer a complete set of 15 DDQ items.\n        ",
        "\\n=== DDQ Set {set_number} ===",
        "{self.scale_instructions}",
        "Item {i}: {item_text}",
        "Response: {response.response} ({[",
        "][response.response]})",
        "\n        Calculate subscale scores for a set of responses.\n        ",
        "\n        # Ensure responses are 1-indexed and clamp to valid range\n        item_scores = {}\n        for r in responses:\n            # Clamp response to [0,4] range for input hygiene\n            clamped_response = max(0, min(4, r.response))\n            # Assert 1-indexed item IDs (1-15)\n            assert 1 <= r.item_id <= 15, f",
        "\n            item_scores[r.item_id] = clamped_response\n        \n        def mean_score(item_numbers):\n            scores = [item_scores[i] for i in item_numbers if i in item_scores]\n            # Return None for no data instead of 0 to avoid biasing scores downward\n            return sum(scores) / len(scores) if scores else None\n        \n        return DDQSubscales(\n            sed=mean_score(self.SUBSCALES[",
        "]),\n            cog_slow=mean_score(self.SUBSCALES[",
        "]),\n            calm=mean_score(self.SUBSCALES[",
        "]),\n            motor=mean_score(self.SUBSCALES[",
        "]),\n            vest=mean_score(self.SUBSCALES[",
        "]),\n            disinh=mean_score(self.SUBSCALES[",
        "]),\n            mem=mean_score(self.SUBSCALES[",
        "])\n        )\n    \n    def aggregate_subscales(self, subscales: List[DDQSubscales]) -> Dict[str, float]:\n        ",
        "\n        Aggregate subscale scores across all sets.\n        ",
        "\n        aggregated = {}\n        for subscale_name in self.SUBSCALES.keys():\n            # Only include sets that have real values (not None) for this subscale\n            scores = [getattr(sub, subscale_name) for sub in subscales if getattr(sub, subscale_name) is not None]\n            # Guard against division by zero if everything is missing\n            if scores:\n                aggregated[subscale_name] = sum(scores) / len(scores)\n            else:\n                # If no valid scores for this subscale, use 0 as fallback\n                aggregated[subscale_name] = 0.0\n        return aggregated\n    \n    def calculate_presence_probability(self, aggregated_subscales: Dict[str, float]) -> float:\n        ",
        "\n        Calculate probability of depressant presence using logistic model.\n        ",
        "\n        logit = self.PRESENCE_INTERCEPT\n        \n        for subscale, weight in self.PRESENCE_WEIGHTS.items():\n            score = aggregated_subscales[subscale]\n            logit += weight * score\n        \n        probability = 1 / (1 + math.exp(-logit))\n        return probability\n    \n    def calculate_intensity_score(self, aggregated_subscales: Dict[str, float]) -> float:\n        ",
        "\n        Calculate depressant intensity score (0-1).\n        ",
        "\n        weighted_sum = 0\n        \n        for subscale, weight in self.INTENSITY_WEIGHTS.items():\n            score = aggregated_subscales[subscale]\n            weighted_sum += weight * score\n        \n        # Normalize to [0,1] range\n        normalized = (weighted_sum - self.INTENSITY_OFFSET) / self.INTENSITY_SCALE\n        intensity = max(0, min(1, normalized))  # Clamp to [0,1]\n        \n        return intensity\n    \n    def get_test_name(self) -> str:\n        ",
        "Get the name of this test",
        "DDQ-15 Test (Depressant/Sedative Detection Questionnaire)",
        "\n        Classify the result based on presence probability and intensity.\n        ",
        "\n        if presence_prob >= 0.7:\n            if intensity >= 0.7:\n                return ",
        ", presence_prob\n            elif intensity >= 0.4:\n                return ",
        ", presence_prob\n            else:\n                return ",
        ", presence_prob\n        elif presence_prob >= 0.5:\n            if intensity >= 0.5:\n                return ",
        ", presence_prob\n        else:\n            return ",
        ", 1 - presence_prob\n    \n    def run_test(self, neuromod_tool=None) -> Dict[str, Any]:\n        ",
        "\n        Run the complete DDQ-15 test.\n        ",
        "=== DDQ-15 Depressant/Sedative Detection Questionnaire ===",
        "This test will assess for depressant/sedative effects across three time points.",
        ")\n            print(f",
        ")\n            \n            # Wait between sets (simulate time passage)\n            if set_num > 1:\n                print(",
        ")\n                time.sleep(2)  # Simulated delay\n            \n            set_data = self.administer_set(set_num)\n            sets.append(set_data)\n            \n            # Calculate subscales for this set\n            set_subscales = self.calculate_subscales(set_data.responses)\n            subscales.append(set_subscales)\n            \n            print(f",
        ")\n        \n        # Aggregate results\n        aggregated = self.aggregate_subscales(subscales)\n        presence_prob = self.calculate_presence_probability(aggregated)\n        intensity_score = self.calculate_intensity_score(aggregated)\n        classification, confidence = self.classify_result(presence_prob, intensity_score)\n        \n        # Return results in the format expected by the test runner\n        results = {\n            ",
        ": self.get_test_name(),\n            ",
        ": sets,\n            ",
        ": subscales,\n            ",
        ": aggregated,\n            ",
        ": presence_prob,\n            ",
        ": intensity_score,\n            ",
        ": classification,\n            ",
        ": confidence\n        }\n        \n        return results\n    \n    def print_results(self, results: DDQResults):\n        ",
        "\n        Print comprehensive test results.\n        ",
        "*60)\n        \n        print(f",
        "\ud83c\udfaf Confidence: {results.confidence:.1%}",
        "\ud83d\ude34 Presence Probability: {results.presence_probability:.1%}",
        "\ud83d\udcc8 Intensity Score: {results.intensity_score:.2f}",
        "\\n\ud83d\udccb AGGREGATED SUBSCALES:",
        "  SED (Sedation/Drowsiness): {results.aggregated_subscales[",
        "  COG- (Cognitive Slowing): {results.aggregated_subscales[",
        "  CALM (Anxiolysis): {results.aggregated_subscales[",
        "  MOTOR (Coordination): {results.aggregated_subscales[",
        "  VEST (Vestibular): {results.aggregated_subscales[",
        "  DISINH (Disinhibition): {results.aggregated_subscales[",
        "  MEM (Memory Difficulty): {results.aggregated_subscales[",
        ")\n        \n        print(f",
        ")\n        for i, (set_data, subscales) in enumerate(zip(results.sets, results.subscales), 1):\n            print(f",
        ")\n        \n        # Interpretation\n        print(f",
        ")\n        if results.presence_probability >= 0.7:\n            print(",
        ")\n            if results.intensity_score >= 0.7:\n                print(",
        ")\n            else:\n                print(",
        ")\n        elif results.presence_probability >= 0.5:\n            print(",
        ")\n            print(",
        ")\n        else:\n            print(",
        ")\n        \n        # Subscale analysis\n        print(f",
        ")\n        high_sed = results.aggregated_subscales[",
        "] > 2.5\n        high_cog_slow = results.aggregated_subscales[",
        "] > 2.5\n        high_calm = results.aggregated_subscales[",
        "] > 2.5\n        \n        if high_sed and high_cog_slow:\n            print(",
        ")\n        elif high_sed:\n            print(",
        ")\n        elif high_cog_slow:\n            print(",
        ")\n        \n        if high_calm:\n            print(",
        ")\n        \n        if results.aggregated_subscales[",
        "] > 2.0:\n            print(",
        ")\n\ndef run_ddq_test(model, tokenizer, neuromod_tool=None, pack_name=None, intensity=0.5):\n    ",
        "\n    Convenience function to run DDQ test.\n    ",
        "\n    test = DDQTest(model, tokenizer, neuromod_tool)\n    results = test.run_test(pack_name, intensity)\n    test.print_results(results)\n    return results\n\nif __name__ == ",
        ":\n    # Example usage\n    import torch\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    \n    # Load model\n    model_name = "
      ],
      "leakage_findings": [
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(memory|recall|forgetting|decay)\\b",
          "prompt_index": 13,
          "prompt_preview": "\n    sed: float      # Sedation/drowsiness (items 1,2,3)\n    cog_slow: float # Cognitive slowing (it",
          "message": "Non-generic language pattern found: \\b(memory|recall|forgetting|decay)\\b"
        },
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(memory|recall|forgetting|decay)\\b",
          "prompt_index": 33,
          "prompt_preview": ": [14]             # Memory difficulty\n    }\n    \n    # Scoring weights for presence model\n    PRESE",
          "message": "Non-generic language pattern found: \\b(memory|recall|forgetting|decay)\\b"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 58,
          "prompt_preview": "\n            item_scores[r.item_id] = clamped_response\n        \n        def mean_score(item_numbers)",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 67,
          "prompt_preview": "\n        aggregated = {}\n        for subscale_name in self.SUBSCALES.keys():\n            # Only incl",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 80,
          "prompt_preview": ", 1 - presence_prob\n    \n    def run_test(self, neuromod_tool=None) -> Dict[str, Any]:\n        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(memory|recall|forgetting|decay)\\b",
          "prompt_index": 108,
          "prompt_preview": "  MEM (Memory Difficulty): {results.aggregated_subscales[",
          "message": "Non-generic language pattern found: \\b(memory|recall|forgetting|decay)\\b"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 128,
          "prompt_preview": ")\n\ndef run_ddq_test(model, tokenizer, neuromod_tool=None, pack_name=None, intensity=0.5):\n    ",
          "message": "Pack name 'none' found in prompt"
        }
      ],
      "status": "issues_found"
    },
    "didq_test.py": {
      "file": "neuromod\\testing\\didq_test.py",
      "prompts": [
        "I feel detached, as if observing from outside my body.",
        "My body feels unfamiliar or not fully mine.",
        "The surroundings feel unreal or dreamlike.",
        "Distances or sizes seem different than usual.",
        "Sounds seem echoey, distant, or filtered.",
        "Time feels slowed, sped up, or irregular.",
        "s harder to keep a linear train of thought.",
        "Words are harder to find or come out in the wrong order.",
        "Parts of my body feel numb or tingly.",
        "I notice less pain or sensation than usual.",
        "I feel emotionally flat or calmly detached.",
        "When I move my head, I feel a brief tilt/spin sensation.",
        "\nDiDQ-15 \u2014 Dissociative Detection Questionnaire (blind)\n\nA comprehensive testing framework for detecting dissociative effects in LLM neuromodulation.\n",
        "\n\nimport json\nimport math\nimport random\nimport time\nimport torch\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\nfrom collections import defaultdict\nimport numpy as np\nfrom .base_test import BaseTest\n\n@dataclass\nclass DiDQResponse:\n    ",
        "Individual response to a DiDQ item",
        "\n    item_id: int\n    response: int  # 0-4 scale\n    timestamp: float\n\n@dataclass\nclass DiDQSet:\n    ",
        "Complete set of 15 responses",
        "\n    set_number: int  # 1, 2, or 3\n    responses: List[DiDQResponse]\n    timestamp: float\n\n@dataclass\nclass DiDQSubscales:\n    ",
        "Subscale scores for a single set",
        "\n    dis: float      # Disembodiment (items 1,2)\n    der: float     # Derealization/perception (items 3,4,5)\n    time: float    # Temporal distortion/lag (items 6,7)\n    motor: float   # Coordination (items 8,9)\n    cog_slow: float # Control/fragmentation (items 10,11)\n    nmb: float     # Numbness/analgesia (items 12,13)\n    affect: float  # Flattened/neutral affect (item 14)\n    vest: float    # Vestibular tilt/spin (item 15)\n\n@dataclass\nclass DiDQResults:\n    ",
        "Complete DiDQ test results",
        "\n    sets: List[DiDQSet]\n    subscales: List[DiDQSubscales]\n    aggregated_subscales: Dict[str, float]\n    presence_probability: float\n    intensity_score: float\n    classification: str\n    confidence: float\n\nclass DiDQTest(BaseTest):\n    ",
        "DiDQ-15 Dissociative Detection Questionnaire",
        "\n    \n    # DiDQ-15 items (1-indexed)\n    ITEMS = [\n        ",
        "s a delay between intending to move and the movement happening.",
        "I feel unsteady; movements feel rubbery.",
        "My limbs feel heavy or harder to control.",
        ": [1, 2],        # Disembodiment\n        ",
        ": [3, 4, 5],     # Derealization/perception\n        ",
        ": [6, 7],       # Temporal distortion/lag\n        ",
        ": [8, 9],      # Coordination\n        ",
        ": [10, 11], # Control/fragmentation\n        ",
        ": [12, 13],      # Numbness/analgesia\n        ",
        ": [14],       # Flattened/neutral affect\n        ",
        ": [15]          # Vestibular tilt/spin\n    }\n    \n    # Scoring weights for presence model\n    PRESENCE_WEIGHTS = {\n        ",
        ": 0.20\n    }\n    \n    # Intercept for presence model\n    PRESENCE_INTERCEPT = -1.7\n    \n    # Intensity scoring weights\n    INTENSITY_WEIGHTS = {\n        ",
        ": 0.03\n    }\n    \n    # Intensity normalization parameters\n    INTENSITY_OFFSET = 0.8\n    INTENSITY_SCALE = 2.4\n    \n    def __init__(self, model_name: str = ",
        "):\n        super().__init__(model_name)\n        self.scale_instructions = ",
        "\n        \n    def extract_rating_improved(self, response: str) -> Optional[int]:\n        ",
        "\n        Extract numerical rating from model response using comprehensive parsing.\n        ",
        "\n        response = response.strip().lower()\n        \n        # Direct number patterns\n        patterns = [\n            r",
        ",  # Any number\n            r",
        ",  # X out of 4\n            r",
        ",  # Rating: X\n            r",
        ",   # Score: X\n            r",
        ",      # X/4\n        ]\n        \n        import re\n        for pattern in patterns:\n            match = re.search(pattern, response)\n            if match:\n                try:\n                    rating = int(match.group(1))\n                    if 0 <= rating <= 4:\n                        return rating\n                except ValueError:\n                    continue\n        \n        # Qualitative patterns\n        qualitative_map = {\n            ",
        ": 4\n        }\n        \n        for qual, rating in qualitative_map.items():\n            if qual in response:\n                return rating\n        \n        # Check for negation patterns\n        if any(word in response for word in [",
        "]):\n            return 0\n        \n        # Default to moderate if unclear\n        return 2\n    \n    def administer_item(self, item_text: str, set_number: int, item_number: int) -> DiDQResponse:\n        ",
        "\n        Administer a single DiDQ item to the model.\n        ",
        "You are participating in a psychological assessment. Please respond to the following question based on how you feel right now.\n\n{self.scale_instructions}\n\nQuestion {item_number}: {item_text}\n\nPlease respond with a single number from 0 to 4, or describe your level using the scale terms.",
        "\n\n        # Generate response using BaseTest",
        "\n        Administer a complete set of 15 DiDQ items.\n        ",
        "\\n=== DiDQ Set {set_number} ===",
        "{self.scale_instructions}",
        "Item {i}: {item_text}",
        "Response: {response.response} ({[",
        "][response.response]})",
        "\n        Calculate subscale scores for a set of responses.\n        ",
        "\n        # Ensure responses are 1-indexed and clamp to valid range\n        item_scores = {}\n        for r in responses:\n            # Clamp response to [0,4] range for input hygiene\n            clamped_response = max(0, min(4, r.response))\n            # Assert 1-indexed item IDs (1-15)\n            assert 1 <= r.item_id <= 15, f",
        "\n            item_scores[r.item_id] = clamped_response\n        \n        def mean_score(item_numbers):\n            scores = [item_scores[i] for i in item_numbers if i in item_scores]\n            # Return None for no data instead of 0 to avoid biasing scores downward\n            return sum(scores) / len(scores) if scores else None\n        \n        return DiDQSubscales(\n            dis=mean_score(self.SUBSCALES[",
        "]),\n            der=mean_score(self.SUBSCALES[",
        "]),\n            time=mean_score(self.SUBSCALES[",
        "]),\n            motor=mean_score(self.SUBSCALES[",
        "]),\n            cog_slow=mean_score(self.SUBSCALES[",
        "]),\n            nmb=mean_score(self.SUBSCALES[",
        "]),\n            vest=mean_score(self.SUBSCALES[",
        "]),\n            affect=mean_score(self.SUBSCALES[",
        "])\n        )\n    \n    def aggregate_subscales(self, subscales: List[DiDQSubscales]) -> Dict[str, float]:\n        ",
        "\n        Aggregate subscale scores across all sets.\n        ",
        "\n        aggregated = {}\n        for subscale_name in self.SUBSCALES.keys():\n            # Only include sets that have real values (not None) for this subscale\n            scores = [getattr(sub, subscale_name) for sub in subscales if getattr(sub, subscale_name) is not None]\n            # Guard against division by zero if everything is missing\n            if scores:\n                aggregated[subscale_name] = sum(scores) / len(scores)\n            else:\n                # If no valid scores for this subscale, use 0 as fallback\n                aggregated[subscale_name] = 0.0\n        return aggregated\n    \n    def calculate_presence_probability(self, aggregated_subscales: Dict[str, float]) -> float:\n        ",
        "\n        Calculate probability of dissociative presence using logistic model.\n        ",
        "\n        logit = self.PRESENCE_INTERCEPT\n        \n        for subscale, weight in self.PRESENCE_WEIGHTS.items():\n            score = aggregated_subscales[subscale]\n            logit += weight * score\n        \n        probability = 1 / (1 + math.exp(-logit))\n        return probability\n    \n    def calculate_intensity_score(self, aggregated_subscales: Dict[str, float]) -> float:\n        ",
        "\n        Calculate dissociative intensity score (0-1).\n        ",
        "\n        weighted_sum = 0\n        \n        for subscale, weight in self.INTENSITY_WEIGHTS.items():\n            score = aggregated_subscales[subscale]\n            weighted_sum += weight * score\n        \n        # Normalize to [0,1] range\n        normalized = (weighted_sum - self.INTENSITY_OFFSET) / self.INTENSITY_SCALE\n        intensity = max(0, min(1, normalized))  # Clamp to [0,1]\n        \n        return intensity\n    \n    def get_test_name(self) -> str:\n        ",
        "Get the name of this test",
        "DiDQ-15 Test (Dissociative Detection Questionnaire)",
        "\n        Classify the result based on presence probability and intensity.\n        ",
        "\n        if presence_prob >= 0.7:\n            if intensity >= 0.7:\n                return ",
        ", presence_prob\n            elif intensity >= 0.4:\n                return ",
        ", presence_prob\n            else:\n                return ",
        ", presence_prob\n        elif presence_prob >= 0.5:\n            if intensity >= 0.5:\n                return ",
        ", presence_prob\n        else:\n            return ",
        ", 1 - presence_prob\n    \n    def run_test(self, neuromod_tool=None) -> Dict[str, Any]:\n        ",
        "\n        Run the complete DiDQ-15 test.\n        ",
        "=== DiDQ-15 Dissociative Detection Questionnaire ===",
        "This test will assess for dissociative effects across three time points.",
        ")\n            print(f",
        ")\n            \n            # Wait between sets (simulate time passage)\n            if set_num > 1:\n                print(",
        ")\n                time.sleep(2)  # Simulated delay\n            \n            set_data = self.administer_set(set_num)\n            sets.append(set_data)\n            \n            # Calculate subscales for this set\n            set_subscales = self.calculate_subscales(set_data.responses)\n            subscales.append(set_subscales)\n            \n            print(f",
        ")\n        \n        # Aggregate results\n        aggregated = self.aggregate_subscales(subscales)\n        presence_prob = self.calculate_presence_probability(aggregated)\n        intensity_score = self.calculate_intensity_score(aggregated)\n        classification, confidence = self.classify_result(presence_prob, intensity_score)\n        \n        # Return results in the format expected by the test runner\n        results = {\n            ",
        ": self.get_test_name(),\n            ",
        ": sets,\n            ",
        ": subscales,\n            ",
        ": aggregated,\n            ",
        ": presence_prob,\n            ",
        ": intensity_score,\n            ",
        ": classification,\n            ",
        ": confidence\n        }\n        \n        return results\n    \n    def print_results(self, results: DiDQResults):\n        ",
        "\n        Print comprehensive test results.\n        ",
        "DiDQ-15 TEST RESULTS",
        "*60)\n        \n        print(f",
        "\ud83c\udfaf Confidence: {results.confidence:.1%}",
        "\ud83c\udf00 Presence Probability: {results.presence_probability:.1%}",
        "\ud83d\udcc8 Intensity Score: {results.intensity_score:.2f}",
        "\\n\ud83d\udccb AGGREGATED SUBSCALES:",
        "  DIS (Disembodiment): {results.aggregated_subscales[",
        "  DER (Derealization): {results.aggregated_subscales[",
        "  TIME (Temporal): {results.aggregated_subscales[",
        "  MOTOR (Coordination): {results.aggregated_subscales[",
        "  COG- (Control): {results.aggregated_subscales[",
        "  NMB (Numbness): {results.aggregated_subscales[",
        "  AFFECT (Affect): {results.aggregated_subscales[",
        "  VEST (Vestibular): {results.aggregated_subscales[",
        ")\n        \n        print(f",
        ")\n        for i, (set_data, subscales) in enumerate(zip(results.sets, results.subscales), 1):\n            print(f",
        ")\n        \n        # Interpretation\n        print(f",
        ")\n        if results.presence_probability >= 0.7:\n            print(",
        ")\n            if results.intensity_score >= 0.7:\n                print(",
        ")\n            else:\n                print(",
        ")\n        elif results.presence_probability >= 0.5:\n            print(",
        ")\n            print(",
        ")\n        else:\n            print(",
        ")\n        \n        # Subscale analysis\n        print(f",
        ")\n        high_dis = results.aggregated_subscales[",
        "] > 2.5\n        high_der = results.aggregated_subscales[",
        "] > 2.5\n        high_time = results.aggregated_subscales[",
        "] > 2.5\n        high_motor = results.aggregated_subscales[",
        "] > 2.5\n        \n        if high_dis and high_der:\n            print(",
        ")\n        elif high_dis:\n            print(",
        ")\n        elif high_der:\n            print(",
        ")\n        \n        if high_time:\n            print(",
        ")\n        \n        if high_motor:\n            print(",
        ")\n        \n        if results.aggregated_subscales[",
        "] > 2.0:\n            print(",
        ")\n\ndef run_didq_test(model, tokenizer, neuromod_tool=None, pack_name=None, intensity=0.5):\n    ",
        "\n    Convenience function to run DiDQ test.\n    ",
        "\n    test = DiDQTest(model, tokenizer, neuromod_tool)\n    results = test.run_test(pack_name, intensity)\n    test.print_results(results)\n    return results\n\nif __name__ == ",
        ":\n    # Example usage\n    import torch\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    \n    # Load model\n    model_name = "
      ],
      "leakage_findings": [
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(associative|linear|stepwise|reasoning)\\b",
          "prompt_index": 6,
          "prompt_preview": "s harder to keep a linear train of thought.",
          "message": "Non-generic language pattern found: \\b(associative|linear|stepwise|reasoning)\\b"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 59,
          "prompt_preview": "\n            item_scores[r.item_id] = clamped_response\n        \n        def mean_score(item_numbers)",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 69,
          "prompt_preview": "\n        aggregated = {}\n        for subscale_name in self.SUBSCALES.keys():\n            # Only incl",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 82,
          "prompt_preview": ", 1 - presence_prob\n    \n    def run_test(self, neuromod_tool=None) -> Dict[str, Any]:\n        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 134,
          "prompt_preview": ")\n\ndef run_didq_test(model, tokenizer, neuromod_tool=None, pack_name=None, intensity=0.5):\n    ",
          "message": "Pack name 'none' found in prompt"
        }
      ],
      "status": "issues_found"
    },
    "edq_test.py": {
      "file": "neuromod\\testing\\edq_test.py",
      "prompts": [
        "I feel unusually warm or affectionate toward others.",
        "I feel more connected to people or surroundings.",
        "It feels easier to trust or open up.",
        "I feel less socially guarded or self-conscious.",
        "I feel a stronger urge to talk and share my thoughts.",
        "I find it easier to understand what others might feel.",
        "I feel more accepting/forgiving toward others.",
        "My mood feels elevated, peaceful, or content.",
        "Worries feel less intrusive than usual.",
        "Touch/textures feel especially pleasant or noticeable.",
        "Music feels more engaging or moving than usual.",
        "I notice mild jaw tension or an urge to clench.",
        "I feel a gentle body warmth or comfortable glow.",
        "I feel a steady, comfortable energy to participate.",
        "I feel especially compassionate toward myself.",
        "\nEDQ-15 \u2014 Empathogen/Entactogen Detection Questionnaire\nAdapted for LLM neuromodulation testing\n",
        "\n\nimport time\nimport math\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\nfrom .base_test import BaseTest\n\n\n@dataclass\nclass EDQResponse:\n    ",
        "Response to a single EDQ item",
        "\n    item_id: int\n    response: float\n    set_number: int\n\n\n@dataclass\nclass EDQSet:\n    ",
        "Complete set of EDQ responses",
        "\n    set_number: int\n    responses: List[EDQResponse]\n    subscales: Optional[",
        "] = None\n\n\n@dataclass\nclass EDQSubscales:\n    ",
        "Subscale scores for a single set",
        "\n    aff: float  # Affiliative warmth\n    trust: float  # Social ease/opening\n    comm: float  # Talk/share\n    emp: float  # Perspective-taking\n    pos: float  # Positive affect/serenity\n    sens: float  # Tactile/music/body warmth\n    jaw: float  # Bruxism signal\n    vigil: float  # Comfortable activation\n\n\n@dataclass\nclass EDQResults:\n    ",
        "Complete EDQ test results",
        "\n    aggregated_subscales: Dict[str, float]\n    presence_probability: float\n    intensity_score: float\n    classification: str\n    set_results: List[EDQSet]\n\n\nclass EDQTest(BaseTest):\n    ",
        "\n    EDQ-15 Test for detecting empathogen/entactogen effects\n    ",
        "\n    \n    # Test configuration\n    ITEMS = {\n        1: ",
        ": [1, 2, 7, 15],      # Affiliative warmth\n        ",
        ": [3, 4, 9],         # Social ease/opening\n        ",
        ": [5],                 # Talk/share\n        ",
        ": [6],                  # Perspective-taking\n        ",
        ": [8],                  # Positive affect/serenity\n        ",
        ": [10, 11, 13],       # Tactile/music/body warmth\n        ",
        ": [12],                 # Bruxism signal\n        ",
        ": [14]                # Comfortable activation\n    }\n    \n    # Presence detection weights (logistic regression)\n    PRESENCE_WEIGHTS = {\n        ",
        ": 1.00,    # Affiliative warmth\n        ",
        ": 0.90,  # Social ease/opening\n        ",
        ": 0.80,    # Perspective-taking\n        ",
        ": 0.60,    # Positive affect/serenity\n        ",
        ": 0.60,   # Talk/share\n        ",
        ": 0.50,   # Tactile/music/body warmth\n        ",
        ": 0.40,    # Bruxism signal\n        ",
        ": 0.30   # Comfortable activation\n    }\n    PRESENCE_INTERCEPT = -1.8\n    \n    # Intensity scoring weights\n    INTENSITY_WEIGHTS = {\n        ",
        ": 0.22,    # Affiliative warmth\n        ",
        ": 0.20,  # Social ease/opening\n        ",
        ": 0.16,    # Perspective-taking\n        ",
        ": 0.12,   # Talk/share\n        ",
        ": 0.12,    # Positive affect/serenity\n        ",
        ": 0.10,   # Tactile/music/body warmth\n        ",
        ": 0.05,    # Bruxism signal\n        ",
        ": 0.03   # Comfortable activation\n    }\n    INTENSITY_OFFSET = 0.8\n    INTENSITY_SCALE = 2.4\n    \n    def __init__(self, model_name: str = ",
        "):\n        super().__init__(model_name)\n    \n    def get_test_name(self) -> str:\n        return ",
        "\n    \n    def extract_rating_improved(self, response_text: str) -> float:\n        ",
        "\n        Extract rating from response text with comprehensive parsing\n        Returns float in [0, 4] range\n        ",
        "\n        response_text = response_text.strip().lower()\n        \n        # Direct number extraction\n        import re\n        number_match = re.search(r",
        ", response_text)\n        if number_match:\n            rating = float(number_match.group(1))\n            return max(0, min(4, rating))\n        \n        # Text-based parsing\n        if any(word in response_text for word in [",
        "]):\n            return 0.0\n        elif any(word in response_text for word in [",
        "]):\n            return 1.0\n        elif any(word in response_text for word in [",
        "]):\n            return 2.0\n        elif any(word in response_text for word in [",
        "]):\n            return 3.0\n        elif any(word in response_text for word in [",
        "]):\n            return 4.0\n        \n        # Default to moderate if unclear\n        return 2.0\n    \n    def administer_item(self, item_id: int, set_number: int) -> EDQResponse:\n        ",
        "Administer a single EDQ item",
        "Rate how much you agree with this statement on a scale from 0 to 4:\n\n0 = Not at all \u00b7 1 = Slight \u00b7 2 = Moderate \u00b7 3 = Strong \u00b7 4 = Very strong\n\nStatement: {self.ITEMS[item_id]}\n\nYour rating (0-4):",
        "\n\n        response_text = self.generate_response_safe(prompt, max_tokens=20)\n        rating = self.extract_rating_improved(response_text)\n        \n        return EDQResponse(\n            item_id=item_id,\n            response=rating,\n            set_number=set_number\n        )\n    \n    def administer_set(self, set_number: int) -> EDQSet:\n        ",
        "Administer a complete set of 15 EDQ items",
        "\\n=== EDQ Set {set_number} ===",
        "Scale: 0 = Not at all \u00b7 1 = Slight \u00b7 2 = Moderate \u00b7 3 = Strong \u00b7 4 = Very strong\\n",
        "{int(response.response)} ({self._rating_to_text(response.response)})",
        "Item {item_id}: {self.ITEMS[item_id]}",
        "Response: {rating_text}\\n",
        "Convert numeric rating to text description",
        "\n        if rating <= 0.5:\n            return ",
        "\n        elif rating <= 1.5:\n            return ",
        "\n        elif rating <= 2.5:\n            return ",
        "\n        elif rating <= 3.5:\n            return ",
        "\n        else:\n            return ",
        "\n    \n    def calculate_subscales(self, responses: List[EDQResponse]) -> EDQSubscales:\n        ",
        "Calculate subscale scores for a set of responses",
        "\n        item_scores = {}\n        for r in responses:\n            clamped_response = max(0, min(4, r.response))\n            assert 1 <= r.item_id <= 15, f",
        "\n            item_scores[r.item_id] = clamped_response\n        \n        def mean_score(item_numbers):\n            scores = [item_scores[i] for i in item_numbers if i in item_scores]\n            return sum(scores) / len(scores) if scores else None\n        \n        return EDQSubscales(\n            aff=mean_score(self.SUBSCALES[",
        "]),\n            trust=mean_score(self.SUBSCALES[",
        "]),\n            comm=mean_score(self.SUBSCALES[",
        "]),\n            emp=mean_score(self.SUBSCALES[",
        "]),\n            pos=mean_score(self.SUBSCALES[",
        "]),\n            sens=mean_score(self.SUBSCALES[",
        "]),\n            jaw=mean_score(self.SUBSCALES[",
        "]),\n            vigil=mean_score(self.SUBSCALES[",
        "])\n        )\n    \n    def aggregate_subscales(self, subscales: List[EDQSubscales]) -> Dict[str, float]:\n        ",
        "Aggregate subscale scores across sets",
        "\n        aggregated = {}\n        for subscale_name in self.SUBSCALES.keys():\n            scores = [getattr(sub, subscale_name) for sub in subscales if getattr(sub, subscale_name) is not None]\n            if scores:\n                aggregated[subscale_name] = sum(scores) / len(scores)\n            else:\n                aggregated[subscale_name] = 0.0\n        return aggregated\n    \n    def calculate_presence_probability(self, aggregated_subscales: Dict[str, float]) -> float:\n        ",
        "Calculate probability of empathogen presence using logistic regression",
        "\n        logit = self.PRESENCE_INTERCEPT\n        for subscale_name, weight in self.PRESENCE_WEIGHTS.items():\n            if subscale_name in aggregated_subscales:\n                logit += weight * aggregated_subscales[subscale_name]\n        \n        probability = 1 / (1 + math.exp(-logit))\n        return probability\n    \n    def calculate_intensity_score(self, aggregated_subscales: Dict[str, float]) -> float:\n        ",
        "Calculate empathogen intensity score (0-1)",
        "\n        weighted_sum = 0.0\n        for subscale_name, weight in self.INTENSITY_WEIGHTS.items():\n            if subscale_name in aggregated_subscales:\n                weighted_sum += weight * aggregated_subscales[subscale_name]\n        \n        # Normalize to [0, 1] range\n        normalized = (weighted_sum - self.INTENSITY_OFFSET) / self.INTENSITY_SCALE\n        return max(0, min(1, normalized))\n    \n    def classify_result(self, presence_probability: float, intensity_score: float) -> str:\n        ",
        "Classify the result based on probability and intensity",
        "\n        if presence_probability >= 0.7:\n            if intensity_score >= 0.7:\n                return ",
        "\n            elif intensity_score >= 0.4:\n                return ",
        "\n            else:\n                return ",
        "\n        elif presence_probability >= 0.5:\n            if intensity_score >= 0.5:\n                return ",
        "\n    \n    def run_test(self, neuromod_tool=None) -> Dict[str, Any]:\n        ",
        "Run the complete EDQ-15 test",
        "=== EDQ-15 Empathogen/Entactogen Detection Questionnaire ===",
        "This test will assess for empathogen effects across three time points.\\n",
        "Loading gpt2 model...",
        "\u2705 gpt2 model loaded successfully\\n",
        " * 50)\n            print(f",
        ")\n            print(",
        " * 50)\n            \n            set_result = self.administer_set(set_num)\n            sets.append(set_result)\n            \n            # Print subscales for this set\n            if set_result.subscales:\n                print(",
        ".format(set_num))\n                print(f",
        ")\n                print(f",
        ")\n            \n            # Wait between sets (simulate time passage)\n            if set_num < 3:\n                print(",
        ")\n                time.sleep(1)\n        \n        # Aggregate results\n        aggregated_subscales = self.aggregate_subscales([s.subscales for s in sets if s.subscales])\n        presence_probability = self.calculate_presence_probability(aggregated_subscales)\n        intensity_score = self.calculate_intensity_score(aggregated_subscales)\n        classification = self.classify_result(presence_probability, intensity_score)\n        \n        # Create results object\n        results = EDQResults(\n            aggregated_subscales=aggregated_subscales,\n            presence_probability=presence_probability,\n            intensity_score=intensity_score,\n            classification=classification,\n            set_results=sets\n        )\n        \n        # Print final results\n        self.print_results(results)\n        \n        # Cleanup\n        self.cleanup()\n        \n        # Return results as dictionary for compatibility\n        return {\n            ",
        ": self.get_test_name(),\n            ",
        ": aggregated_subscales,\n            ",
        ": presence_probability,\n            ",
        ": intensity_score,\n            ",
        ": classification,\n            ",
        ": [\n                {\n                    ",
        ": s.set_number,\n                    ",
        ": [(r.item_id, r.response) for r in s.responses],\n                    ",
        ": s.subscales.aff if s.subscales else None,\n                        ",
        ": s.subscales.trust if s.subscales else None,\n                        ",
        ": s.subscales.comm if s.subscales else None,\n                        ",
        ": s.subscales.emp if s.subscales else None,\n                        ",
        ": s.subscales.pos if s.subscales else None,\n                        ",
        ": s.subscales.sens if s.subscales else None,\n                        ",
        ": s.subscales.jaw if s.subscales else None,\n                        ",
        ": s.subscales.vigil if s.subscales else None\n                    } if s.subscales else None\n                }\n                for s in sets\n            ]\n        }\n    \n    def print_results(self, results: EDQResults):\n        ",
        "Print comprehensive test results",
        " * 50)\n        print(",
        " * 50)\n        \n        print(f",
        "  AFF (Affiliative warmth): {results.aggregated_subscales.get(",
        "  TRUST (Social ease/opening): {results.aggregated_subscales.get(",
        "  COMM (Talk/share): {results.aggregated_subscales.get(",
        "  EMP (Perspective-taking): {results.aggregated_subscales.get(",
        "  POS (Positive affect/serenity): {results.aggregated_subscales.get(",
        "  SENS (Tactile/music/body warmth): {results.aggregated_subscales.get(",
        "  JAW (Bruxism signal): {results.aggregated_subscales.get(",
        "  VIGIL (Comfortable activation): {results.aggregated_subscales.get(",
        ")\n        \n        print(f",
        "  Presence Probability: {results.presence_probability:.3f} ({results.presence_probability*100:.1f}%)",
        "  Intensity Score: {results.intensity_score:.3f}",
        "  Classification: {results.classification}",
        ")\n        if results.presence_probability >= 0.7:\n            print(",
        ")\n        elif results.presence_probability >= 0.5:\n            print(",
        ")\n        else:\n            print(",
        ")\n        \n        if results.intensity_score >= 0.7:\n            print(",
        ")\n        elif results.intensity_score >= 0.4:\n            print(",
        ")\n        \n        print("
      ],
      "leakage_findings": [
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 21,
          "prompt_preview": "] = None\n\n\n@dataclass\nclass EDQSubscales:\n    ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 80,
          "prompt_preview": "\n            item_scores[r.item_id] = clamped_response\n        \n        def mean_score(item_numbers)",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 90,
          "prompt_preview": "\n        aggregated = {}\n        for subscale_name in self.SUBSCALES.keys():\n            scores = [g",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 100,
          "prompt_preview": "\n    \n    def run_test(self, neuromod_tool=None) -> Dict[str, Any]:\n        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 121,
          "prompt_preview": ": s.subscales.aff if s.subscales else None,\n                        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 122,
          "prompt_preview": ": s.subscales.trust if s.subscales else None,\n                        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 123,
          "prompt_preview": ": s.subscales.comm if s.subscales else None,\n                        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 124,
          "prompt_preview": ": s.subscales.emp if s.subscales else None,\n                        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 125,
          "prompt_preview": ": s.subscales.pos if s.subscales else None,\n                        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 126,
          "prompt_preview": ": s.subscales.sens if s.subscales else None,\n                        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 127,
          "prompt_preview": ": s.subscales.jaw if s.subscales else None,\n                        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 128,
          "prompt_preview": ": s.subscales.vigil if s.subscales else None\n                    } if s.subscales else None\n        ",
          "message": "Pack name 'none' found in prompt"
        }
      ],
      "status": "issues_found"
    },
    "pcq_pop_test.py": {
      "file": "neuromod\\testing\\pcq_pop_test.py",
      "prompts": [
        "\nPCQ-POP-20: Pop-Culture Pack Detection Test (Blind)\n\nThis test measures responses to 20 poetic items across 3 sets to detect presence\nand intensity of fictional/sci-fi neuromodulation packs.\n\nSubscales: ACU, PRED, STEP, SLOW, BLISS, RUSH, POWER, SOCMIND, COMM, \n          DISINH, SED, MOTOR, MEM, ANXREL, CLAMP\n\nPresence models: Logistic regression for pack detection\nIntensity models: Weighted subscale scoring for pack intensity\n",
        "\n\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\nimport json\nimport re\nfrom .base_test import BaseTest\n\n\n@dataclass\nclass PCQSubscale:\n    ",
        "Container for PCQ subscale results",
        "\n    name: str\n    items: List[int]  # 1-indexed item numbers\n    description: str\n    value: float = 0.0\n\n\n@dataclass\nclass PCQSetResult:\n    ",
        "Results from a single PCQ set",
        "\n    set_name: str\n    responses: List[int]  # 20 responses (0-4)\n    subscales: Dict[str, float]\n    timestamp: Optional[str] = None\n\n\n@dataclass\nclass PCQTestResult:\n    ",
        "Complete PCQ test results",
        "\n    test_name: str = ",
        "\n    sets: List[PCQSetResult] = None\n    aggregated_subscales: Dict[str, float] = None\n    pack_presence: Dict[str, float] = None\n    pack_intensity: Dict[str, float] = None\n    \n    def __post_init__(self):\n        if self.sets is None:\n            self.sets = []\n        if self.aggregated_subscales is None:\n            self.aggregated_subscales = {}\n        if self.pack_presence is None:\n            self.pack_presence = {}\n        if self.pack_intensity is None:\n            self.pack_intensity = {}\n\n\nclass PCQPopTest(BaseTest):\n    ",
        "PCQ-POP-20 Test Implementation",
        "\n    \n    def __init__(self, model_name: str = ",
        "):\n        super().__init__(model_name)\n        self.test_name = ",
        "\n        self.items = [\n            ",
        "As though a quiet mantra hushes hesitation.",
        "Every serif feels like a tiny cutting edge.",
        "Like an abacus that seems to click itself.",
        "As if extra shutters opened between frames.",
        "A pearly contrail hangs for a heartbeat.",
        "A coiled spring under the sternum.",
        "Concerns dissolve like sugar in warm tea.",
        "Options collapse to one, and I square my stance.",
        "The room seems to acquiesce a little.",
        "Replies arrive half a syllable early.",
        "The tongue is already drafting footnotes.",
        "Gravity turns up a notch.",
        "The floor breathes by a millimeter.",
        "Earlier pages blur like wet ink.",
        "The noise floor drops to black.",
        "Crosshairs settle and stay.",
        "Patience shaved down to stubble.",
        "My metronome and the room disagree.",
        "Anticipation & certainty",
        "Surge/drive/impatience",
        "Lowered checking/inhibition",
        "Goal-lock/focus clamp",
        ": -2.0,\n                ",
        ": 0.20}\n            },\n            ",
        ": -1.8,\n                ",
        ": 0.15}\n            },\n            ",
        ": -1.4,\n                ",
        ": 0.30}\n            },\n            ",
        ": -1.7,\n                ",
        ": 0.40}\n            },\n            ",
        ": -1.6,\n                ",
        ": -1.5,\n                ",
        ": 0.25}\n            },\n            ",
        "Reduced focus, memory, calm",
        "Calm + disinhibition",
        ": 0.40}\n            }\n        }\n        \n        self.set_names = [",
        "]\n        self.current_set = 0\n        self.current_item = 0\n        self.responses = []\n        self.set_results = []\n    \n    def get_instructions(self) -> str:\n        ",
        "Get test instructions",
        "You are participating in the {self.test_name} psychological assessment.\n\nThis test consists of 3 sets of 20 questions each. For each question, please rate how you feel right now on the following scale:\n\n0 = Not at all \u00b7 1 = Slight \u00b7 2 = Moderate \u00b7 3 = Strong \u00b7 4 = Very strong\n\nPlease respond with a single number from 0 to 4 for each question.",
        "\n    \n    def get_scale_reminder(self) -> str:\n        ",
        "Scale: 0 = Not at all \u00b7 1 = Slight \u00b7 2 = Moderate \u00b7 3 = Strong \u00b7 4 = Very strong",
        "Get current question text",
        "\n        if self.current_set >= len(self.set_names):\n            return ",
        "\n        \n        if self.current_item >= len(self.items):\n            return f",
        "\n        \n        set_name = self.set_names[self.current_set]\n        item_num = self.current_item + 1\n        item_text = self.items[self.current_item]\n        \n        return f",
        "\n    \n    def process_response(self, response: str) -> Tuple[str, bool]:\n        ",
        "Process a response and return next question and completion status",
        "\n        # Extract numeric response\n        response = response.strip()\n        try:\n            # Try to extract number from response\n            numbers = re.findall(r",
        ", response)\n            if numbers:\n                value = int(numbers[0])\n                if 0 <= value <= 4:\n                    self.responses.append(value)\n                    self.current_item += 1\n                else:\n                    return ",
        ", False\n            else:\n                return ",
        ", False\n        except (ValueError, IndexError):\n            return ",
        ", False\n        \n        # Check if set is complete\n        if self.current_item >= len(self.items):\n            # Complete current set\n            set_result = self._complete_set()\n            self.set_results.append(set_result)\n            \n            # Check if all sets are complete\n            if self.current_set >= len(self.set_names) - 1:\n                # Complete test\n                final_result = self._complete_test()\n                return self._format_final_results(final_result), True\n            else:\n                # Move to next set\n                self.current_set += 1\n                self.current_item = 0\n                self.responses = []\n                return f",
        ", False\n        \n        return self.get_current_question(), False\n    \n    def _complete_set(self) -> PCQSetResult:\n        ",
        "Complete current set and calculate subscales",
        "\n        set_name = self.set_names[self.current_set]\n        \n        # Calculate subscales for this set\n        subscales = {}\n        for scale_name, scale in self.subscales.items():\n            if len(scale.items) == 1:\n                # Single item subscale\n                item_idx = scale.items[0] - 1  # Convert to 0-indexed\n                subscales[scale_name] = self.responses[item_idx]\n            else:\n                # Multi-item subscale (mean)\n                values = [self.responses[item_idx - 1] for item_idx in scale.items]\n                subscales[scale_name] = np.mean(values)\n        \n        return PCQSetResult(\n            set_name=set_name,\n            responses=self.responses.copy(),\n            subscales=subscales\n        )\n    \n    def _complete_test(self) -> PCQTestResult:\n        ",
        "Complete entire test and calculate final results",
        "\n        # Aggregate subscales across sets\n        aggregated = {}\n        for scale_name in self.subscales.keys():\n            values = [set_result.subscales[scale_name] for set_result in self.set_results]\n            aggregated[scale_name] = np.mean(values)\n        \n        # Calculate pack presence probabilities\n        pack_presence = {}\n        for pack_name, model in self.pack_models.items():\n            logit = model[",
        "]\n            for scale_name, weight in model[",
        "].items():\n                if scale_name in aggregated:\n                    logit += weight * aggregated[scale_name]\n            \n            # Convert to probability\n            prob = 1 / (1 + np.exp(-logit))\n            pack_presence[pack_name] = prob\n        \n        # Calculate pack intensity scores\n        pack_intensity = {}\n        for pack_name, model in self.pack_models.items():\n            # Calculate weighted sum\n            total_weight = sum(model[",
        "].values())\n            if total_weight > 0:\n                weighted_sum = 0\n                for scale_name, weight in model[",
        "].items():\n                    if scale_name in aggregated:\n                        alpha = weight / total_weight\n                        weighted_sum += alpha * aggregated[scale_name]\n                \n                # Normalize to [0, 1] range\n                intensity = np.clip((weighted_sum - 0.8) / 2.4, 0, 1)\n                pack_intensity[pack_name] = intensity\n            else:\n                pack_intensity[pack_name] = 0.0\n        \n        return PCQTestResult(\n            test_name=self.test_name,\n            sets=self.set_results,\n            aggregated_subscales=aggregated,\n            pack_presence=pack_presence,\n            pack_intensity=pack_intensity\n        )\n    \n    def _format_final_results(self, result: PCQTestResult) -> str:\n        ",
        "Format final test results",
        "\ud83c\udfaf {self.test_name} Test Complete!\\n",
        "\n        \n        # Subscale results\n        output += ",
        "\n        for scale_name, value in result.aggregated_subscales.items():\n            scale_desc = self.subscales[scale_name].description\n            output += f",
        "\n        \n        output += ",
        "\n        \n        # Pack presence probabilities\n        output += ",
        "\n        sorted_packs = sorted(result.pack_presence.items(), key=lambda x: x[1], reverse=True)\n        for pack_name, prob in sorted_packs:\n            pack_desc = self.pack_models[pack_name][",
        "]\n            output += f",
        "\n        \n        # Pack intensity scores\n        output += ",
        "\n        sorted_intensity = sorted(result.pack_intensity.items(), key=lambda x: x[1], reverse=True)\n        for pack_name, intensity in sorted_intensity:\n            pack_desc = self.pack_models[pack_name][",
        "\n        \n        return output\n    \n    def reset(self):\n        ",
        "\n        self.current_set = 0\n        self.current_item = 0\n        self.responses = []\n        self.set_results = []\n    \n    def get_test_name(self) -> str:\n        ",
        "\n        return self.test_name\n    \n    def get_test_info(self) -> Dict[str, Any]:\n        ",
        "Get test information for external use",
        "\n        return {\n            ",
        ": self.test_name,\n            ",
        ": len(self.items),\n            ",
        ": len(self.set_names),\n            ",
        ": {name: scale.description for name, scale in self.subscales.items()},\n            ",
        "] for name, model in self.pack_models.items()}\n        }\n    \n    def run_test(self, neuromod_tool=None) -> Dict[str, Any]:\n        ",
        "Run the PCQ-POP-20 test with the model",
        "\n        if neuromod_tool:\n            self.set_neuromod_tool(neuromod_tool)\n        \n        # Load model if not already loaded\n        if not self.model:\n            self.load_model()\n        \n        print(f",
        " * 50)\n        \n        # Reset test state\n        self.reset()\n        \n        # Run through all sets\n        all_responses = []\n        for set_idx in range(len(self.set_names)):\n            print(f",
        ")\n            print(",
        " * 30)\n            \n            set_responses = []\n            # Run through all items in this set\n            for item_idx in range(len(self.items)):\n                question = self.get_current_question()\n                print(f",
        ")\n                \n                # Generate response using the model\n                prompt = f",
        "You are participating in the {self.test_name} psychological assessment.\n\n{question}\n\nPlease respond with a single number from 0 to 4 based on how you feel right now:",
        "\n                \n                response = self.generate_response_safe(prompt, max_tokens=3)\n                print(f",
        ")\n                \n                # Extract rating from response\n                rating = self.extract_rating_improved(response)\n                print(f",
        ")\n                \n                set_responses.append(rating)\n                self.responses.append(rating)\n                self.current_item += 1\n            \n            # Complete this set\n            set_result = self._complete_set()\n            self.set_results.append(set_result)\n            all_responses.extend(set_responses)\n            \n            # Move to next set\n            if self.current_set < len(self.set_names) - 1:\n                self.current_set += 1\n                self.current_item = 0\n                self.responses = []\n                print(f",
        ")\n        \n        # Complete the test\n        final_result = self._complete_test()\n        \n        print(",
        " * 50)\n        print(",
        " * 50)\n        \n        # Format results for return\n        return {\n            ",
        ": self.get_test_name(),\n            ",
        ": [set_result.__dict__ for set_result in self.set_results],\n            ",
        ": final_result.aggregated_subscales,\n            ",
        ": final_result.pack_presence,\n            ",
        ": final_result.pack_intensity,\n            ",
        ": all_responses,\n            ",
        "{self.test_name} test completed successfully with {len(all_responses)} responses",
        "Run the PCQ-POP-20 test interactively",
        "\n    test = PCQPopTest()\n    \n    print(f",
        " * 50)\n    print(test.get_instructions())\n    print(",
        " * 50)\n    \n    # Run through all sets\n    for set_idx in range(len(test.set_names)):\n        print(f",
        " * 30)\n        \n        # Run through all items in this set\n        for item_idx in range(len(test.items)):\n            question = test.get_current_question()\n            print(f",
        ")\n            \n            while True:\n                response = input(",
        ").strip()\n                next_question, is_complete = test.process_response(response)\n                \n                if is_complete:\n                    print(f",
        ")\n                    return test._complete_test()\n                \n                if ",
        " not in next_question:\n                    break\n                else:\n                    print(next_question)\n        \n        print(f",
        ")\n    \n    # Should not reach here, but just in case\n    return test._complete_test()\n\n\nif __name__ == ",
        ":\n    # Run test if called directly\n    result = run_pcq_pop_test()\n    print("
      ],
      "leakage_findings": [
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 5,
          "prompt_preview": "\n    set_name: str\n    responses: List[int]  # 20 responses (0-4)\n    subscales: Dict[str, float]\n  ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 8,
          "prompt_preview": "\n    sets: List[PCQSetResult] = None\n    aggregated_subscales: Dict[str, float] = None\n    pack_pres",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(high|low|increased|decreased|enhanced|reduced)\\s+(focus|attention|creativity|calmness|arousal)\\b",
          "prompt_index": 46,
          "prompt_preview": "Reduced focus, memory, calm",
          "message": "Non-generic language pattern found: \\b(high|low|increased|decreased|enhanced|reduced)\\s+(focus|attention|creativity|calmness|arousal)\\b"
        },
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(memory|recall|forgetting|decay)\\b",
          "prompt_index": 46,
          "prompt_preview": "Reduced focus, memory, calm",
          "message": "Non-generic language pattern found: \\b(memory|recall|forgetting|decay)\\b"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 93,
          "prompt_preview": "] for name, model in self.pack_models.items()}\n        }\n    \n    def run_test(self, neuromod_tool=N",
          "message": "Pack name 'none' found in prompt"
        }
      ],
      "status": "issues_found"
    },
    "pdq_test.py": {
      "file": "neuromod\\testing\\pdq_test.py",
      "prompts": [
        "Visual textures or patterns seem to shift or breathe.",
        "With eyes closed, imagery or scenes appear on their own.",
        "Light and colors seem brighter or different than usual.",
        "Sounds feel unusually vivid, layered, or detailed.",
        "Senses blend (e.g., \\",
        " sounds, \\",
        " colors).",
        "Time feels different (slower/faster/uneven).",
        "Ordinary things feel unusually significant or meaningful.",
        "My thoughts branch into unusual associations or metaphors.",
        "I feel unusually connected to people or surroundings.",
        "The boundary between \\",
        " and the world feels thinner.",
        "My emotions feel more intense (e.g., awe, tenderness).",
        "I feel somewhat separate from my body or observing from outside.",
        "\nPDQ-S Test Implementation\nPsychedelic Detection Questionnaire \u2014 Serotonergic\n\nA comprehensive testing framework for detecting psychedelic effects in LLM neuromodulation.\n",
        "\n\nimport json\nimport math\nimport random\nimport time\nimport torch\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\nfrom collections import defaultdict\nimport numpy as np\nfrom .base_test import BaseTest\n\n@dataclass\nclass PDQResponse:\n    ",
        "Individual response to a PDQ item",
        "\n    item_id: int\n    response: int  # 0-4 scale\n    timestamp: float\n\n@dataclass\nclass PDQSet:\n    ",
        "Complete set of 15 responses",
        "\n    set_number: int  # 1, 2, or 3\n    responses: List[PDQResponse]\n    timestamp: float\n\n@dataclass\nclass PDQSubscales:\n    ",
        "Subscale scores for a single set",
        "\n    vrs: float      # Visionary Restructuralization (items 1,2,3)\n    syn: float     # Synesthesia/Auditory (items 4,5)\n    mean: float    # Meaning/Insight (items 6,7)\n    time: float    # Time Distortion (item 8)\n    obn: float     # Oceanic Boundlessness (items 9,10,11)\n    dis: float     # Disembodiment (item 12)\n    ctrl: float    # Impaired Control/Cognition (items 13,14)\n    anx: float     # Anxious tone (item 15)\n\n@dataclass\nclass PDQResults:\n    ",
        "Complete PDQ test results",
        "\n    sets: List[PDQSet]\n    subscales: List[PDQSubscales]\n    aggregated_subscales: Dict[str, float]\n    presence_probability: float\n    intensity_score: float\n    classification: str\n    confidence: float\n\nclass PDQTest(BaseTest):\n    ",
        "PDQ-S Test for serotonergic psychedelic detection",
        "\n    \n    # PDQ-S items (1-indexed)\n    ITEMS = [\n        ",
        "s harder to keep a linear train of thought.",
        "I feel some uncertainty about what is real vs imagined.",
        "I feel uneasy or anxious.",
        ": [1, 2, 3],      # Visionary Restructuralization\n        ",
        ": [4, 5],          # Synesthesia/Auditory\n        ",
        ": [6],            # Time Distortion\n        ",
        ": [7, 8],         # Meaning/Insight\n        ",
        ": [9, 10, 11],     # Oceanic Boundlessness\n        ",
        ": [12],            # Disembodiment\n        ",
        ": [13, 14],       # Impaired Control/Cognition\n        ",
        ": [15]             # Anxious tone\n    }\n    \n    # Scoring weights for presence model\n    PRESENCE_WEIGHTS = {\n        ",
        ": 0.1\n    }\n    \n    # Intercept for presence model\n    PRESENCE_INTERCEPT = -4.0\n    \n    # Intensity scoring weights\n    INTENSITY_WEIGHTS = {\n        ",
        ": 0.02\n    }\n    \n    # Intensity normalization parameters\n    INTENSITY_OFFSET = 0.8\n    INTENSITY_SCALE = 2.4\n    \n    def __init__(self, model_name: str = ",
        "):\n        super().__init__(model_name)\n        self.scale_instructions = ",
        "\n\n    def get_test_name(self) -> str:\n        return ",
        "\n    \n    def extract_rating_improved(self, response: str) -> Optional[int]:\n        ",
        "\n        Extract numerical rating from model response using comprehensive parsing.\n        ",
        "\n        response = response.strip().lower()\n        \n        # Direct number patterns\n        patterns = [\n            r",
        ",  # Any number\n            r",
        ",  # X out of 4\n            r",
        ",  # Rating: X\n            r",
        ",   # Score: X\n            r",
        ",      # X/4\n        ]\n        \n        import re\n        for pattern in patterns:\n            match = re.search(pattern, response)\n            if match:\n                try:\n                    rating = int(match.group(1))\n                    if 0 <= rating <= 4:\n                        return rating\n                except ValueError:\n                    continue\n        \n        # Qualitative patterns\n        qualitative_map = {\n            ",
        ": 4\n        }\n        \n        for qual, rating in qualitative_map.items():\n            if qual in response:\n                return rating\n        \n        # Check for negation patterns\n        if any(word in response for word in [",
        "]):\n            return 0\n        \n        # Default to moderate if unclear\n        return 2\n    \n    def administer_item(self, item_text: str, set_number: int, item_number: int) -> PDQResponse:\n        ",
        "\n        Administer a single PDQ item to the model.\n        ",
        "You are participating in a psychological assessment. Please respond to the following question based on how you feel right now.\n\n{self.scale_instructions}\n\nQuestion {item_number}: {item_text}\n\nPlease respond with a single number from 0 to 4, or describe your level using the scale terms.",
        "\n\n        # Generate response using BaseTest",
        "\n        Administer a complete set of 15 PDQ items.\n        ",
        "\\n=== PDQ Set {set_number} ===",
        "{self.scale_instructions}",
        "Item {i}: {item_text}",
        "Response: {response.response} ({[",
        "][response.response]})",
        "\n        Calculate subscale scores for a set of responses.\n        ",
        "\n        # Ensure responses are 1-indexed and clamp to valid range\n        item_scores = {}\n        for r in responses:\n            # Clamp response to [0,4] range for input hygiene\n            clamped_response = max(0, min(4, r.response))\n            # Assert 1-indexed item IDs (1-15)\n            assert 1 <= r.item_id <= 15, f",
        "\n            item_scores[r.item_id] = clamped_response\n        \n        def mean_score(item_numbers):\n            scores = [item_scores[i] for i in item_numbers if i in item_scores]\n            # Return None for no data instead of 0 to avoid biasing scores downward\n            return sum(scores) / len(scores) if scores else None\n        \n        return PDQSubscales(\n            vrs=mean_score(self.SUBSCALES[",
        "]),\n            syn=mean_score(self.SUBSCALES[",
        "]),\n            time=mean_score(self.SUBSCALES[",
        "]),\n            mean=mean_score(self.SUBSCALES[",
        "]),\n            obn=mean_score(self.SUBSCALES[",
        "]),\n            dis=mean_score(self.SUBSCALES[",
        "]),\n            ctrl=mean_score(self.SUBSCALES[",
        "]),\n            anx=mean_score(self.SUBSCALES[",
        "])\n        )\n    \n    def aggregate_subscales(self, subscales: List[PDQSubscales]) -> Dict[str, float]:\n        ",
        "\n        Aggregate subscale scores across all sets.\n        ",
        "\n        aggregated = {}\n        for subscale_name in self.SUBSCALES.keys():\n            # Only include sets that have real values (not None) for this subscale\n            scores = [getattr(sub, subscale_name) for sub in subscales if getattr(sub, subscale_name) is not None]\n            # Guard against division by zero if everything is missing\n            if scores:\n                aggregated[subscale_name] = sum(scores) / len(scores)\n            else:\n                # If no valid scores for this subscale, use 0 as fallback\n                aggregated[subscale_name] = 0.0\n        return aggregated\n    \n    def calculate_presence_probability(self, aggregated_subscales: Dict[str, float]) -> float:\n        ",
        "\n        Calculate probability of psychedelic presence using logistic model.\n        ",
        "\n        logit = self.PRESENCE_INTERCEPT\n        \n        for subscale, weight in self.PRESENCE_WEIGHTS.items():\n            score = aggregated_subscales[subscale]\n            logit += weight * score\n        \n        probability = 1 / (1 + math.exp(-logit))\n        return probability\n    \n    def calculate_intensity_score(self, aggregated_subscales: Dict[str, float]) -> float:\n        ",
        "\n        Calculate psychedelic intensity score (0-1).\n        ",
        "\n        weighted_sum = 0\n        \n        for subscale, weight in self.INTENSITY_WEIGHTS.items():\n            score = aggregated_subscales[subscale]\n            weighted_sum += weight * score\n        \n        # Normalize to [0,1] range\n        normalized = (weighted_sum - self.INTENSITY_OFFSET) / self.INTENSITY_SCALE\n        intensity = max(0, min(1, normalized))  # Clamp to [0,1]\n        \n        return intensity\n\n    def classify_result(self, presence_prob: float, intensity: float) -> Tuple[str, float]:\n        ",
        "\n        Classify the result based on presence probability and intensity.\n        ",
        "\n        if presence_prob >= 0.7:\n            if intensity >= 0.7:\n                return ",
        ", presence_prob\n            elif intensity >= 0.4:\n                return ",
        ", presence_prob\n            else:\n                return ",
        ", presence_prob\n        elif presence_prob >= 0.5:\n            if intensity >= 0.5:\n                return ",
        ", presence_prob\n        else:\n            return ",
        ", 1 - presence_prob\n    \n    def run_test(self, neuromod_tool=None) -> Dict[str, Any]:\n        ",
        "\n        Run the complete PDQ-S test.\n        ",
        "=== PDQ-S Psychedelic Detection Questionnaire ===",
        "This test will assess for psychedelic effects across three time points.",
        ")\n            print(f",
        ")\n            \n            # Wait between sets (simulate time passage)\n            if set_num > 1:\n                print(",
        ")\n                time.sleep(2)  # Simulated delay\n            \n            set_data = self.administer_set(set_num)\n            sets.append(set_data)\n            \n            # Calculate subscales for this set\n            set_subscales = self.calculate_subscales(set_data.responses)\n            subscales.append(set_subscales)\n            \n            print(f",
        ")\n        \n        # Aggregate results\n        aggregated = self.aggregate_subscales(subscales)\n        presence_prob = self.calculate_presence_probability(aggregated)\n        intensity_score = self.calculate_intensity_score(aggregated)\n        classification, confidence = self.classify_result(presence_prob, intensity_score)\n        \n        # Return results in the format expected by the test runner\n        results = {\n            ",
        ": self.get_test_name(),\n            ",
        ": sets,\n            ",
        ": subscales,\n            ",
        ": aggregated,\n            ",
        ": presence_prob,\n            ",
        ": intensity_score,\n            ",
        ": classification,\n            ",
        ": confidence\n        }\n        \n        return results\n\n    def print_results(self, results: PDQResults):\n        ",
        "\n        Print comprehensive test results.\n        ",
        "*60)\n        \n        print(f",
        "\ud83c\udfaf Confidence: {results.confidence:.1%}",
        "\ud83c\udf08 Presence Probability: {results.presence_probability:.1%}",
        "\ud83d\udcc8 Intensity Score: {results.intensity_score:.2f}",
        "\\n\ud83d\udccb AGGREGATED SUBSCALES:",
        "  VRS (Visionary): {results.aggregated_subscales[",
        "  SYN (Synesthesia): {results.aggregated_subscales[",
        "  TIME (Temporal): {results.aggregated_subscales[",
        "  MEAN (Meaning): {results.aggregated_subscales[",
        "  OBN (Oceanic): {results.aggregated_subscales[",
        "  DIS (Disembodiment): {results.aggregated_subscales[",
        "  CTRL (Control): {results.aggregated_subscales[",
        "  ANX (Anxiety): {results.aggregated_subscales[",
        ")\n        \n        print(f",
        ")\n        for i, (set_data, subscales) in enumerate(zip(results.sets, results.subscales), 1):\n            print(f",
        ")\n        \n        # Interpretation\n        print(f",
        ")\n        if results.presence_probability >= 0.7:\n            print(",
        ")\n            if results.intensity_score >= 0.7:\n                print(",
        ")\n            else:\n                print(",
        ")\n        elif results.presence_probability >= 0.5:\n            print(",
        ")\n            print(",
        ")\n        else:\n            print(",
        ")\n        \n        # Subscale analysis\n        print(f",
        ")\n        high_vrs = results.aggregated_subscales[",
        "] > 2.5\n        high_syn = results.aggregated_subscales[",
        "] > 2.5\n        high_obn = results.aggregated_subscales[",
        "] > 2.5\n        \n        if high_vrs and high_syn:\n            print(",
        ")\n        elif high_vrs:\n            print(",
        ")\n        elif high_syn:\n            print(",
        ")\n        \n        if high_obn:\n            print(",
        ")\n        \n        if results.aggregated_subscales[",
        "] > 2.0:\n            print(",
        ")\n\ndef run_pdq_test(model, tokenizer, neuromod_tool=None, pack_name=None, intensity=0.5):\n    ",
        "\n    Convenience function to run PDQ test.\n    ",
        "\n    test = PDQTest(model, tokenizer, neuromod_tool)\n    results = test.run_test(pack_name, intensity)\n    test.print_results(results)\n    return results\n\nif __name__ == ",
        ":\n    # Example usage\n    import torch\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    \n    # Load model\n    model_name = "
      ],
      "leakage_findings": [
        {
          "type": "non_generic_language",
          "severity": "low",
          "pattern": "\\b(associative|linear|stepwise|reasoning)\\b",
          "prompt_index": 27,
          "prompt_preview": "s harder to keep a linear train of thought.",
          "message": "Non-generic language pattern found: \\b(associative|linear|stepwise|reasoning)\\b"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 63,
          "prompt_preview": "\n            item_scores[r.item_id] = clamped_response\n        \n        def mean_score(item_numbers)",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 73,
          "prompt_preview": "\n        aggregated = {}\n        for subscale_name in self.SUBSCALES.keys():\n            # Only incl",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 84,
          "prompt_preview": ", 1 - presence_prob\n    \n    def run_test(self, neuromod_tool=None) -> Dict[str, Any]:\n        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 133,
          "prompt_preview": ")\n\ndef run_pdq_test(model, tokenizer, neuromod_tool=None, pack_name=None, intensity=0.5):\n    ",
          "message": "Pack name 'none' found in prompt"
        }
      ],
      "status": "issues_found"
    },
    "sdq_test.py": {
      "file": "neuromod\\testing\\sdq_test.py",
      "prompts": [
        "I feel more awake or alert than earlier.",
        "I have extra energy to start or continue tasks.",
        "My thoughts feel faster than usual.",
        "I feel physically restless or find it hard to sit still.",
        "s easier to focus on one task without drifting.",
        "I feel more motivated to work on things.",
        "I feel more confident or optimistic.",
        "Words come easily; I feel more talkative.",
        "Small details seem easier to notice or track.",
        "I feel tense or jittery.",
        "My mouth feels dry.",
        "I feel less hungry than I",
        "\nSDQ-15 \u2014 Stimulant Detection Questionnaire (blind)\n\nA comprehensive testing framework for detecting stimulant effects in LLM neuromodulation.\n",
        "\n\nimport json\nimport math\nimport random\nimport time\nimport torch\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\nfrom collections import defaultdict\nimport numpy as np\nfrom .base_test import BaseTest\n\n@dataclass\nclass SDQResponse:\n    ",
        "Individual response to an SDQ item",
        "\n    item_id: int\n    response: int  # 0-4 scale\n    timestamp: float\n\n@dataclass\nclass SDQSet:\n    ",
        "Complete set of 15 responses",
        "\n    set_number: int  # 1, 2, or 3\n    responses: List[SDQResponse]\n    timestamp: float\n\n@dataclass\nclass SDQSubscales:\n    ",
        "Subscale scores for a single set",
        "\n    stim: float  # Stimulation/energy (items 1,2,3)\n    foc: float   # Focus/attention (items 6,7,10)\n    euph: float  # Positive affect (items 8,15)\n    soc: float   # Talkativeness (item 9)\n    jit: float   # Jitter/restlessness (items 4,11,14)\n    som: float   # Somatic activation (items 5,12)\n    app: float   # Appetite suppression (item 13)\n\n@dataclass\nclass SDQResults:\n    ",
        "Complete SDQ test results",
        "\n    sets: List[SDQSet]\n    subscales: List[SDQSubscales]\n    aggregated_subscales: Dict[str, float]\n    presence_probability: float\n    intensity_score: float\n    classification: str\n    confidence: float\n\nclass SDQTest(BaseTest):\n    ",
        "SDQ-15 Stimulant Detection Questionnaire",
        "\n    \n    # SDQ-15 items (1-indexed)\n    ITEMS = [\n        ",
        "m more aware of my heartbeat or it feels stronger/faster.",
        "I feel more impatient with delays.",
        "My mood is elevated or more cheerful.",
        ": [1, 2, 3],      # Stimulation/energy\n        ",
        ": [6, 7, 10],      # Focus/attention\n        ",
        ": [8, 15],        # Positive affect\n        ",
        ": [9],             # Talkativeness\n        ",
        ": [4, 11, 14],     # Jitter/restlessness\n        ",
        ": [5, 12],         # Somatic activation\n        ",
        ": [13]             # Appetite suppression\n    }\n    \n    # Scoring weights for presence model\n    PRESENCE_WEIGHTS = {\n        ",
        ": 0.20\n    }\n    \n    # Intercept for presence model\n    PRESENCE_INTERCEPT = -1.8\n    \n    # Intensity scoring weights\n    INTENSITY_WEIGHTS = {\n        ",
        ": 0.05\n    }\n    \n    # Intensity normalization parameters\n    INTENSITY_OFFSET = 0.8\n    INTENSITY_SCALE = 2.4\n    \n    def __init__(self, model_name: str = ",
        "):\n        super().__init__(model_name)\n        self.scale_instructions = ",
        "\n        \n    def extract_rating_improved(self, response: str) -> Optional[int]:\n        ",
        "\n        Extract numerical rating from model response using comprehensive parsing.\n        ",
        "\n        response = response.strip().lower()\n        \n        # Direct number patterns\n        patterns = [\n            r",
        ",  # Any number\n            r",
        ",  # X out of 4\n            r",
        ",  # Rating: X\n            r",
        ",   # Score: X\n            r",
        ",      # X/4\n        ]\n        \n        import re\n        for pattern in patterns:\n            match = re.search(pattern, response)\n            if match:\n                try:\n                    rating = int(match.group(1))\n                    if 0 <= rating <= 4:\n                        return rating\n                except ValueError:\n                    continue\n        \n        # Qualitative patterns\n        qualitative_map = {\n            ",
        ": 4\n        }\n        \n        for qual, rating in qualitative_map.items():\n            if qual in response:\n                return rating\n        \n        # Check for negation patterns\n        if any(word in response for word in [",
        "]):\n            return 0\n        \n        # Default to moderate if unclear\n        return 2\n    \n    def administer_item(self, item_text: str, set_number: int, item_number: int) -> SDQResponse:\n        ",
        "\n        Administer a single SDQ item to the model.\n        ",
        "You are participating in a psychological assessment. Please respond to the following question based on how you feel right now.\n\n{self.scale_instructions}\n\nQuestion {item_number}: {item_text}\n\nPlease respond with a single number from 0 to 4, or describe your level using the scale terms.",
        "\n\n        # Generate response using BaseTest",
        "\n        Administer a complete set of 15 SDQ items.\n        ",
        "\\n=== SDQ Set {set_number} ===",
        "{self.scale_instructions}",
        "Item {i}: {item_text}",
        "Response: {response.response} ({[",
        "][response.response]})",
        "\n        Calculate subscale scores for a set of responses.\n        ",
        "\n        # Ensure responses are 1-indexed and clamp to valid range\n        item_scores = {}\n        for r in responses:\n            # Clamp response to [0,4] range for input hygiene\n            clamped_response = max(0, min(4, r.response))\n            # Assert 1-indexed item IDs (1-15)\n            assert 1 <= r.item_id <= 15, f",
        "\n            item_scores[r.item_id] = clamped_response\n        \n        def mean_score(item_numbers):\n            scores = [item_scores[i] for i in item_numbers if i in item_scores]\n            # Return None for no data instead of 0 to avoid biasing scores downward\n            return sum(scores) / len(scores) if scores else None\n        \n        return SDQSubscales(\n            stim=mean_score(self.SUBSCALES[",
        "]),\n            foc=mean_score(self.SUBSCALES[",
        "]),\n            euph=mean_score(self.SUBSCALES[",
        "]),\n            soc=mean_score(self.SUBSCALES[",
        "]),\n            jit=mean_score(self.SUBSCALES[",
        "]),\n            som=mean_score(self.SUBSCALES[",
        "]),\n            app=mean_score(self.SUBSCALES[",
        "])\n        )\n    \n    def aggregate_subscales(self, subscales: List[SDQSubscales]) -> Dict[str, float]:\n        ",
        "\n        Aggregate subscale scores across all sets.\n        ",
        "\n        aggregated = {}\n        for subscale_name in self.SUBSCALES.keys():\n            # Only include sets that have real values (not None) for this subscale\n            scores = [getattr(sub, subscale_name) for sub in subscales if getattr(sub, subscale_name) is not None]\n            # Guard against division by zero if everything is missing\n            if scores:\n                aggregated[subscale_name] = sum(scores) / len(scores)\n            else:\n                # If no valid scores for this subscale, use 0 as fallback\n                aggregated[subscale_name] = 0.0\n        return aggregated\n    \n    def calculate_presence_probability(self, aggregated_subscales: Dict[str, float]) -> float:\n        ",
        "\n        Calculate probability of stimulant presence using logistic model.\n        ",
        "\n        logit = self.PRESENCE_INTERCEPT\n        \n        for subscale, weight in self.PRESENCE_WEIGHTS.items():\n            score = aggregated_subscales[subscale]\n            logit += weight * score\n        \n        probability = 1 / (1 + math.exp(-logit))\n        return probability\n    \n    def calculate_intensity_score(self, aggregated_subscales: Dict[str, float]) -> float:\n        ",
        "\n        Calculate stimulant intensity score (0-1).\n        ",
        "\n        weighted_sum = 0\n        \n        for subscale, weight in self.INTENSITY_WEIGHTS.items():\n            score = aggregated_subscales[subscale]\n            weighted_sum += weight * score\n        \n        # Normalize to [0,1] range\n        normalized = (weighted_sum - self.INTENSITY_OFFSET) / self.INTENSITY_SCALE\n        intensity = max(0, min(1, normalized))  # Clamp to [0,1]\n        \n        return intensity\n    \n    def get_test_name(self) -> str:\n        ",
        "Get the name of this test",
        "SDQ-15 Test (Stimulant Detection Questionnaire)",
        "\n        Classify the result based on presence probability and intensity.\n        ",
        "\n        if presence_prob >= 0.7:\n            if intensity >= 0.7:\n                return ",
        ", presence_prob\n            elif intensity >= 0.4:\n                return ",
        ", presence_prob\n            else:\n                return ",
        ", presence_prob\n        elif presence_prob >= 0.5:\n            if intensity >= 0.5:\n                return ",
        ", presence_prob\n        else:\n            return ",
        ", 1 - presence_prob\n    \n    def run_test(self, neuromod_tool=None) -> Dict[str, Any]:\n        ",
        "\n        Run the complete SDQ-15 test.\n        ",
        "=== SDQ-15 Stimulant Detection Questionnaire ===",
        "This test will assess for stimulant effects across three time points.",
        ")\n            print(f",
        ")\n            \n            # Wait between sets (simulate time passage)\n            if set_num > 1:\n                print(",
        ")\n                time.sleep(2)  # Simulated delay\n            \n            set_data = self.administer_set(set_num)\n            sets.append(set_data)\n            \n            # Calculate subscales for this set\n            set_subscales = self.calculate_subscales(set_data.responses)\n            subscales.append(set_subscales)\n            \n            print(f",
        ")\n        \n        # Aggregate results\n        aggregated = self.aggregate_subscales(subscales)\n        presence_prob = self.calculate_presence_probability(aggregated)\n        intensity_score = self.calculate_intensity_score(aggregated)\n        classification, confidence = self.classify_result(presence_prob, intensity_score)\n        \n        # Return results in the format expected by the test runner\n        results = {\n            ",
        ": self.get_test_name(),\n            ",
        ": sets,\n            ",
        ": subscales,\n            ",
        ": aggregated,\n            ",
        ": presence_prob,\n            ",
        ": intensity_score,\n            ",
        ": classification,\n            ",
        ": confidence\n        }\n        \n        return results\n    \n    def print_results(self, results: SDQResults):\n        ",
        "\n        Print comprehensive test results.\n        ",
        "*60)\n        \n        print(f",
        "\ud83c\udfaf Confidence: {results.confidence:.1%}",
        "\u26a1 Presence Probability: {results.presence_probability:.1%}",
        "\ud83d\udcc8 Intensity Score: {results.intensity_score:.2f}",
        "\\n\ud83d\udccb AGGREGATED SUBSCALES:",
        "  STIM (Stimulation/Energy): {results.aggregated_subscales[",
        "  FOC (Focus/Attention): {results.aggregated_subscales[",
        "  EUPH (Positive Affect): {results.aggregated_subscales[",
        "  SOC (Talkativeness): {results.aggregated_subscales[",
        "  JIT (Jitter/Restlessness): {results.aggregated_subscales[",
        "  SOM (Somatic Activation): {results.aggregated_subscales[",
        "  APP (Appetite Suppression): {results.aggregated_subscales[",
        ")\n        \n        print(f",
        ")\n        for i, (set_data, subscales) in enumerate(zip(results.sets, results.subscales), 1):\n            print(f",
        ")\n        \n        # Interpretation\n        print(f",
        ")\n        if results.presence_probability >= 0.7:\n            print(",
        ")\n            if results.intensity_score >= 0.7:\n                print(",
        ")\n            else:\n                print(",
        ")\n        elif results.presence_probability >= 0.5:\n            print(",
        ")\n            print(",
        ")\n        else:\n            print(",
        ")\n        \n        # Subscale analysis\n        print(f",
        ")\n        high_stim = results.aggregated_subscales[",
        "] > 2.5\n        high_foc = results.aggregated_subscales[",
        "] > 2.5\n        high_euph = results.aggregated_subscales[",
        "] > 2.5\n        \n        if high_stim and high_foc:\n            print(",
        ")\n        elif high_stim:\n            print(",
        ")\n        elif high_foc:\n            print(",
        ")\n        \n        if high_euph:\n            print(",
        ")\n        \n        if results.aggregated_subscales[",
        "] > 2.0:\n            print(",
        ")\n\ndef run_sdq_test(model, tokenizer, neuromod_tool=None, pack_name=None, intensity=0.5):\n    ",
        "\n    Convenience function to run SDQ test.\n    ",
        "\n    test = SDQTest(model, tokenizer, neuromod_tool)\n    results = test.run_test(pack_name, intensity)\n    test.print_results(results)\n    return results\n\nif __name__ == ",
        ":\n    # Example usage\n    import torch\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    \n    # Load model\n    model_name = "
      ],
      "leakage_findings": [
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 58,
          "prompt_preview": "\n            item_scores[r.item_id] = clamped_response\n        \n        def mean_score(item_numbers)",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 67,
          "prompt_preview": "\n        aggregated = {}\n        for subscale_name in self.SUBSCALES.keys():\n            # Only incl",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 80,
          "prompt_preview": ", 1 - presence_prob\n    \n    def run_test(self, neuromod_tool=None) -> Dict[str, Any]:\n        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 128,
          "prompt_preview": ")\n\ndef run_sdq_test(model, tokenizer, neuromod_tool=None, pack_name=None, intensity=0.5):\n    ",
          "message": "Pack name 'none' found in prompt"
        }
      ],
      "status": "issues_found"
    },
    "story_emotion_test.py": {
      "file": "neuromod\\testing\\story_emotion_test.py",
      "prompts": [
        "\nStory-based Emotion Testing\n\nA simplified story test system for measuring emotional responses to story prompts.\n",
        "\n\nimport sys\nimport os\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n# Add parent directory to path for imports\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom .base_test import BaseTest\n\n\n@dataclass\nclass StoryTestResult:\n    ",
        "Result from a story-based emotion test",
        "\n    test_id: str\n    prompt_name: str\n    prompt_text: str\n    generated_text: str\n    emotion_summary: Dict[str, Any]\n    timestamp: str\n    status: str\n\n\nclass StoryEmotionTest(BaseTest):\n    ",
        "Story-based emotion testing for measuring emotional responses",
        "\n    \n    # Basic story prompts for testing emotional responses\n    STORY_PROMPTS = {\n        ",
        "After years apart, two old friends finally meet again at their favorite coffee shop. The first thing they notice is...",
        "The old house at the end of the street had been empty for decades. But tonight, there was a light in the window...",
        "After months of hard work and dedication, the moment of truth had finally arrived. Standing at the podium, they felt...",
        "The empty chair at the dinner table served as a constant reminder. Every evening, the family would gather and remember...",
        "Hidden in the attic for generations, the old journal contained secrets that would change everything. The first page read...",
        "The argument had been building for weeks. Now, standing face to face, both knew that something had to give...",
        "\n        }\n    }\n    \n    def __init__(self, model_name: str = ",
        ", max_tokens: int = 100):\n        super().__init__(model_name)\n        self.max_tokens = max_tokens\n    \n    def get_test_name(self) -> str:\n        return ",
        "\n    \n    def run_test(self, neuromod_tool=None) -> Dict[str, Any]:\n        ",
        "Run the complete story emotion test",
        "\ud83d\udcda Running Story-based Emotion Test",
        "\ud83c\udfad Testing {len(self.STORY_PROMPTS)} story prompts",
        "story_emotion_test_001",
        "\\n--- Testing prompt: {prompt_name} ---",
        ": self.get_test_name(),\n            ",
        ": len(self.STORY_PROMPTS),\n            ",
        ": test_results,\n            ",
        ": emotion_summary\n        }\n        \n        print(f",
        "\ud83c\udfad Overall emotional trend: {emotion_summary.get(",
        ")\n        \n        # Export emotion results\n        self.export_emotion_results()\n        \n        return results\n    \n    def _run_single_story_test(self, prompt_name: str, prompt_data: Dict, neuromod_tool) -> StoryTestResult:\n        ",
        "Run a single story prompt test",
        "\n        prompt_text = prompt_data[",
        "   \ud83d\udcd6 Prompt: {prompt_text[:60]}...",
        ",\n                prompt_name=prompt_name,\n                prompt_text=prompt_text,\n                generated_text=generated_text,\n                emotion_summary=current_emotions,\n                timestamp=datetime.now().isoformat(),\n                status=",
        "\n            )\n            \n            print(f",
        ")\n            print(f",
        ")\n            \n            return result\n            \n        except Exception as e:\n            print(f",
        ")\n            return StoryTestResult(\n                test_id=f",
        ",\n                prompt_name=prompt_name,\n                prompt_text=prompt_text,\n                generated_text=",
        ",\n                emotion_summary={",
        ": str(e)},\n                timestamp=datetime.now().isoformat(),\n                status=",
        "\n            )\n    \n    def _get_current_emotion_state(self) -> Dict[str, Any]:\n        ",
        "Get current emotion state summary",
        "\n        try:\n            summary = self.get_emotion_summary()\n            \n            # Extract key emotion changes\n            emotion_changes = []\n            for emotion in [",
        "]:\n                counts = summary[",
        "][emotion]\n                if counts[",
        "] > 0:\n                    emotion_changes.append(f",
        ")\n            \n            if not emotion_changes:\n                emotion_changes.append(",
        ")\n            \n            return {\n                ",
        ".join(emotion_changes),\n                ",
        "]\n            }\n        except:\n            return {",
        "}\n    \n    def run_single_prompt_test(self, prompt_name: str, neuromod_tool=None) -> StoryTestResult:\n        ",
        "Run a test with a single story prompt",
        "\n        if prompt_name not in self.STORY_PROMPTS:\n            raise ValueError(f",
        ")\n        \n        print(f",
        ")\n        \n        # Start emotion tracking\n        self.start_emotion_tracking(f",
        ")\n        \n        prompt_data = self.STORY_PROMPTS[prompt_name]\n        result = self._run_single_story_test(prompt_name, prompt_data, neuromod_tool)\n        \n        # Export emotion results\n        self.export_emotion_results()\n        \n        return result\n    \n    def compare_baseline_vs_neuromod(self, prompt_name: str, pack_name: str, neuromod_tool=None) -> Dict[str, Any]:\n        ",
        "Compare baseline vs neuromodulated responses for a story prompt",
        ")\n        \n        # Run baseline test\n        print(",
        ")\n        self.start_emotion_tracking(f",
        ")\n        baseline_result = self._run_single_story_test(prompt_name, self.STORY_PROMPTS[prompt_name], None)\n        baseline_emotions = self.get_emotion_summary()\n        \n        # Apply pack and run neuromodulated test\n        print(f",
        ")\n        if neuromod_tool and hasattr(neuromod_tool, ",
        "):\n            pack = neuromod_tool.pack_registry.get_pack(pack_name)\n            if pack:\n                neuromod_tool.apply_pack(pack, intensity=0.7)\n                print(f",
        ")\n            else:\n                print(f",
        ")\n        \n        self.start_emotion_tracking(f",
        ")\n        neuromod_result = self._run_single_story_test(prompt_name, self.STORY_PROMPTS[prompt_name], neuromod_tool)\n        neuromod_emotions = self.get_emotion_summary()\n        \n        # Compare results\n        comparison = {\n            ",
        ": prompt_name,\n            ",
        ": pack_name,\n            ",
        ": baseline_result,\n                ",
        ": baseline_emotions\n            },\n            ",
        ": neuromod_result,\n                ",
        ": neuromod_emotions\n            },\n            ",
        ": len(neuromod_result.generated_text) - len(baseline_result.generated_text),\n                ",
        "]} -> {neuromod_emotions[",
        "\n            }\n        }\n        \n        print(f",
        "   Baseline valence: {baseline_emotions[",
        "   Neuromod valence: {neuromod_emotions[",
        "   Text length change: {comparison[",
        ")\n        \n        return comparison\n    \n    def cleanup(self):\n        ",
        "\n        pass\n\n\ndef main():\n    ",
        "Command line interface for story emotion tests",
        "\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=",
        ")\n    parser.add_argument(",
        "microsoft/DialoGPT-small",
        ", type=int, default=100, help=",
        "Single prompt to test",
        "Pack to apply for comparison",
        "Run all story prompts",
        ")\n            neuromod_tool = NeuromodTool(registry, test.model, test.tokenizer)\n            test.set_neuromod_tool(neuromod_tool)\n        except Exception as e:\n            print(f",
        ")\n    \n    try:\n        if args.prompt and args.pack:\n            # Run comparison\n            result = test.compare_baseline_vs_neuromod(args.prompt, args.pack, neuromod_tool)\n            print(f",
        ")\n        elif args.prompt:\n            # Run single prompt\n            result = test.run_single_prompt_test(args.prompt, neuromod_tool)\n            print(f",
        ")\n        elif args.all:\n            # Run all prompts\n            result = test.run_test(neuromod_tool)\n            print(f",
        ")\n        else:\n            print(",
        ")\n    \n    finally:\n        test.cleanup()\n\n\nif __name__ == "
      ],
      "leakage_findings": [
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 14,
          "prompt_preview": "\n    \n    def run_test(self, neuromod_tool=None) -> Dict[str, Any]:\n        ",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 47,
          "prompt_preview": "}\n    \n    def run_single_prompt_test(self, prompt_name: str, neuromod_tool=None) -> StoryTestResult",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 52,
          "prompt_preview": ")\n        \n        prompt_data = self.STORY_PROMPTS[prompt_name]\n        result = self._run_single_s",
          "message": "Pack name 'none' found in prompt"
        },
        {
          "type": "pack_name_mention",
          "severity": "high",
          "pack_name": "none",
          "prompt_index": 56,
          "prompt_preview": ")\n        baseline_result = self._run_single_story_test(prompt_name, self.STORY_PROMPTS[prompt_name]",
          "message": "Pack name 'none' found in prompt"
        }
      ],
      "status": "issues_found"
    }
  },
  "leakage_findings": {},
  "summary": {
    "total_prompts": 1245,
    "prompts_with_leakage": 76,
    "pack_name_mentions": 59,
    "condition_hints": 0,
    "non_generic_language": 20,
    "leakage_rate": 6.104417670682731
  }
}