#!/usr/bin/env python3
"""
Local Test Script for Prediction Server
Test the prediction server locally before Docker deployment
"""

import sys
import os
import time
import requests
import json
from pathlib import Path

# Add the project root to the path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def test_imports():
    """Test if all required modules can be imported"""
    print("üß™ Testing imports...")
    
    try:
        # Test neuromodulation system
        from neuromod import NeuromodTool
        print("‚úÖ NeuromodTool imported successfully")
        
        from neuromod.testing.simple_emotion_tracker import SimpleEmotionTracker
        print("‚úÖ SimpleEmotionTracker imported successfully")
        
        # Test transformers
        import torch
        print(f"‚úÖ PyTorch imported: {torch.__version__}")
        
        from transformers import AutoTokenizer, AutoModelForCausalLM
        print("‚úÖ Transformers imported successfully")
        
        return True
        
    except ImportError as e:
        print(f"‚ùå Import failed: {e}")
        return False

def test_model_loading():
    """Test if we can load a small model locally"""
    print("\nüß™ Testing model loading...")
    
    try:
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        # Use a very small model for testing
        model_name = "microsoft/DialoGPT-small"
        print(f"Loading test model: {model_name}")
        
        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        print("‚úÖ Tokenizer loaded successfully")
        
        # Load model with minimal settings
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            dtype=torch.float32,
            device_map="cpu",  # Use CPU for testing
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        print("‚úÖ Model loaded successfully")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Model loading failed: {e}")
        return False

def test_neuromodulation():
    """Test if neuromodulation system works"""
    print("\nüß™ Testing neuromodulation system...")
    
    try:
        from neuromod import NeuromodTool
        from neuromod.pack_system import PackRegistry
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        # Create mock components for testing
        tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-small")
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # Create a minimal mock model for testing
        class MockModel:
            def __init__(self):
                self.device = "cpu"
        
        mock_model = MockModel()
        
        # Initialize neuromod tool with mock components
        neuromod_tool = NeuromodTool(
            registry=PackRegistry(),
            model=mock_model,
            tokenizer=tokenizer
        )
        print("‚úÖ NeuromodTool initialized")
        
        # Test emotion tracker
        from neuromod.testing.simple_emotion_tracker import SimpleEmotionTracker
        emotion_tracker = SimpleEmotionTracker()
        print("‚úÖ Emotion tracker initialized")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Neuromodulation test failed: {e}")
        return False

def test_prediction_server_code():
    """Test if the prediction server code can be imported and initialized"""
    print("\nüß™ Testing prediction server code...")
    
    try:
        # Import the prediction server functions
        from prediction_server import (
            load_model, 
            apply_neuromodulation, 
            generate_text_with_probes,
            register_probe_hooks
        )
        print("‚úÖ Prediction server functions imported")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Prediction server import failed: {e}")
        return False

def test_flask_server():
    """Test if Flask server can start"""
    print("\nüß™ Testing Flask server...")
    
    try:
        from flask import Flask
        app = Flask(__name__)
        
        @app.route('/test')
        def test():
            return {"status": "ok"}
        
        print("‚úÖ Flask app created successfully")
        return True
        
    except Exception as e:
        print(f"‚ùå Flask test failed: {e}")
        return False

def test_docker_build():
    """Test if Docker build context is correct"""
    print("\nüß™ Testing Docker build context...")
    
    try:
        # Get project root (parent of vertex_container)
        project_root = Path(__file__).parent.parent
        
        # Check if required files exist from project root
        required_files = [
            "neuromod/",
            "packs/",
            "vertex_container/requirements.txt",
            "vertex_container/prediction_server.py"
        ]
        
        for file_path in required_files:
            full_path = project_root / file_path
            if full_path.exists():
                print(f"‚úÖ {file_path} exists")
            else:
                print(f"‚ùå {file_path} missing")
                return False
        
        # Check requirements.txt
        requirements_path = project_root / "vertex_container/requirements.txt"
        with open(requirements_path, "r") as f:
            requirements = f.read()
            if "flask" in requirements.lower():
                print("‚úÖ Flask in requirements.txt")
            else:
                print("‚ùå Flask missing from requirements.txt")
                return False
        
        return True
        
    except Exception as e:
        print(f"‚ùå Docker build context test failed: {e}")
        return False

def run_comprehensive_test():
    """Run all tests"""
    print("üöÄ Starting comprehensive prediction server test...\n")
    
    tests = [
        ("Import Test", test_imports),
        ("Model Loading Test", test_model_loading),
        ("Neuromodulation Test", test_neuromodulation),
        ("Prediction Server Test", test_prediction_server_code),
        ("Flask Test", test_flask_server),
        ("Docker Build Context Test", test_docker_build)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"‚ùå {test_name} crashed: {e}")
            results.append((test_name, False))
    
    # Summary
    print("\n" + "="*50)
    print("üìä TEST RESULTS SUMMARY")
    print("="*50)
    
    passed = 0
    total = len(results)
    
    for test_name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{status} {test_name}")
        if success:
            passed += 1
    
    print(f"\nüéØ Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("üéâ ALL TESTS PASSED! Ready for Docker deployment.")
        return True
    else:
        print("‚ö†Ô∏è Some tests failed. Fix issues before Docker deployment.")
        return False

if __name__ == "__main__":
    # Change to vertex_container directory
    os.chdir(Path(__file__).parent)
    
    success = run_comprehensive_test()
    
    if success:
        print("\nüöÄ Next steps:")
        print("1. Build Docker container: docker build -t neuromod-vertex-ai .")
        print("2. Test container locally: docker run -p 8080:8080 neuromod-vertex-ai")
        print("3. Test endpoints: curl http://localhost:8080/health")
        print("4. Deploy to Vertex AI if all local tests pass")
    else:
        print("\nüîß Fix the failing tests before proceeding with Docker deployment.")
        sys.exit(1)
