# üöÄ Staged Deployment Strategy for Vertex AI

## üéØ **Why Staged Deployment?**

Instead of deploying everything at once and hoping it works, we deploy incrementally to catch issues early.

## üìã **Deployment Stages**

### **Stage 1: Minimal Container (Low Risk)**
**Goal:** Verify basic container functionality
**Time:** 15-30 minutes
**Risk:** Very Low

```bash
# Deploy minimal container with basic functionality
docker build -f Dockerfile.test -t gcr.io/YOUR_PROJECT_ID/neuromod-minimal:latest .
docker push gcr.io/YOUR_PROJECT_ID/neuromod-minimal:latest

# Deploy to Vertex AI with minimal model
MODEL_NAME="microsoft/DialoGPT-small"
```

**Tests:**
- ‚úÖ Container starts
- ‚úÖ Health endpoint responds
- ‚úÖ Basic prediction works
- ‚úÖ No crashes

**Success Criteria:** Container runs without errors

### **Stage 2: Full Container (Medium Risk)**
**Goal:** Verify full neuromodulation system
**Time:** 30-45 minutes
**Risk:** Medium

```bash
# Deploy full container with all features
docker build -t gcr.io/YOUR_PROJECT_ID/neuromod-full:latest .
docker push gcr.io/YOUR_PROJECT_ID/neuromod-full:latest

# Deploy with full neuromodulation
MODEL_NAME="microsoft/DialoGPT-small"
NEUROMODULATION_ENABLED=true
PROBE_SYSTEM_ENABLED=true
EMOTION_TRACKING_ENABLED=true
```

**Tests:**
- ‚úÖ All Stage 1 tests pass
- ‚úÖ Neuromodulation system loads
- ‚úÖ Probe system works
- ‚úÖ Emotion tracking works
- ‚úÖ Pack loading works

**Success Criteria:** Full system functionality verified

### **Stage 3: Production Model (High Risk)**
**Goal:** Deploy with production model
**Time:** 45-60 minutes
**Risk:** High

```bash
# Deploy with production model
MODEL_NAME="meta-llama/Meta-Llama-3.1-8B"
NEUROMODULATION_ENABLED=true
PROBE_SYSTEM_ENABLED=true
EMOTION_TRACKING_ENABLED=true
```

**Tests:**
- ‚úÖ All Stage 2 tests pass
- ‚úÖ Large model loads successfully
- ‚úÖ Memory usage acceptable
- ‚úÖ Performance acceptable
- ‚úÖ All features work with large model

**Success Criteria:** Production-ready deployment

## üõ°Ô∏è **Risk Mitigation at Each Stage**

### **Stage 1 Risk Mitigation**
- **Model Loading:** Use smallest possible model
- **Memory:** Minimal dependencies
- **Network:** Basic connectivity only
- **Rollback:** Easy to revert to previous version

### **Stage 2 Risk Mitigation**
- **Neuromodulation:** Test with simple packs first
- **Probes:** Verify hook registration
- **Emotions:** Test with basic tracking
- **Monitoring:** Enhanced logging

### **Stage 3 Risk Mitigation**
- **Model Size:** Monitor memory usage closely
- **Performance:** Load testing
- **Stability:** Long-running tests
- **Backup:** Keep Stage 2 as fallback

## üîç **Testing Strategy for Each Stage**

### **Stage 1 Testing**
```bash
# Quick health check
curl http://localhost:8080/health

# Basic prediction
curl -X POST http://localhost:8080/predict \
  -H "Content-Type: application/json" \
  -d '{"instances": [{"prompt": "Hello", "max_tokens": 10}]}'

# Check logs
docker logs <container_id>
```

### **Stage 2 Testing**
```bash
# Test probe system
curl http://localhost:8080/probe_status

# Test emotion tracking
curl http://localhost:8080/emotion_status

# Test with neuromodulation
curl -X POST http://localhost:8080/predict \
  -H "Content-Type: application/json" \
  -d '{"instances": [{"prompt": "Hello", "pack_name": "dmt", "max_tokens": 20}]}'
```

### **Stage 3 Testing**
```bash
# Load testing
for i in {1..10}; do
  curl -X POST http://localhost:8080/predict \
    -H "Content-Type: application/json" \
    -d '{"instances": [{"prompt": "Test $i", "max_tokens": 50}]}'
done

# Memory monitoring
docker stats <container_id>

# Performance testing
time curl -X POST http://localhost:8080/predict \
  -H "Content-Type: application/json" \
  -d '{"instances": [{"prompt": "Long prompt", "max_tokens": 100}]}'
```

## üö® **Rollback Strategy**

### **Stage 1 Rollback**
```bash
# Revert to previous container
docker tag gcr.io/YOUR_PROJECT_ID/neuromod-minimal:previous gcr.io/YOUR_PROJECT_ID/neuromod-minimal:latest
docker push gcr.io/YOUR_PROJECT_ID/neuromod-minimal:latest
```

### **Stage 2 Rollback**
```bash
# Revert to Stage 1
docker tag gcr.io/YOUR_PROJECT_ID/neuromod-minimal:latest gcr.io/YOUR_PROJECT_ID/neuromod-full:latest
docker push gcr.io/YOUR_PROJECT_ID/neuromod-full:latest
```

### **Stage 3 Rollback**
```bash
# Revert to Stage 2
docker tag gcr.io/YOUR_PROJECT_ID/neuromod-full:latest gcr.io/YOUR_PROJECT_ID/neuromod-production:latest
docker push gcr.io/YOUR_PROJECT_ID/neuromod-production:latest
```

## üìä **Success Metrics**

### **Stage 1 Metrics**
- Container startup time < 30 seconds
- Health endpoint response time < 1 second
- Basic prediction response time < 5 seconds
- Zero crashes in first hour

### **Stage 2 Metrics**
- All Stage 1 metrics pass
- Neuromodulation pack loading < 10 seconds
- Probe system initialization < 5 seconds
- Emotion tracking response time < 2 seconds

### **Stage 3 Metrics**
- All Stage 2 metrics pass
- Large model loading < 5 minutes
- Memory usage < 80% of container limit
- Prediction response time < 30 seconds
- Zero crashes in first 24 hours

## üîß **Deployment Scripts**

### **Stage 1 Deployment**
```bash
#!/bin/bash
# deploy_stage1.sh

echo "üöÄ Deploying Stage 1: Minimal Container"

# Build minimal container
docker build -f Dockerfile.test -t gcr.io/YOUR_PROJECT_ID/neuromod-minimal:latest .
docker push gcr.io/YOUR_PROJECT_ID/neuromod-minimal:latest

# Deploy to Vertex AI
python -c "
from api.vertex_ai_manager import VertexAIManager
manager = VertexAIManager('YOUR_PROJECT_ID', 'us-central1')
manager.create_custom_endpoint(
    'neuromod-minimal',
    'gcr.io/YOUR_PROJECT_ID/neuromod-minimal:latest',
    model_name='microsoft/DialoGPT-small'
)
"

echo "‚úÖ Stage 1 deployment complete"
```

### **Stage 2 Deployment**
```bash
#!/bin/bash
# deploy_stage2.sh

echo "üöÄ Deploying Stage 2: Full Container"

# Build full container
docker build -t gcr.io/YOUR_PROJECT_ID/neuromod-full:latest .
docker push gcr.io/YOUR_PROJECT_ID/neuromod-full:latest

# Deploy to Vertex AI
python -c "
from api.vertex_ai_manager import VertexAIManager
manager = VertexAIManager('YOUR_PROJECT_ID', 'us-central1')
manager.create_custom_endpoint(
    'neuromod-full',
    'gcr.io/YOUR_PROJECT_ID/neuromod-full:latest',
    model_name='microsoft/DialoGPT-small'
)
"

echo "‚úÖ Stage 2 deployment complete"
```

### **Stage 3 Deployment**
```bash
#!/bin/bash
# deploy_stage3.sh

echo "üöÄ Deploying Stage 3: Production Model"

# Deploy with production model
python -c "
from api.vertex_ai_manager import VertexAIManager
manager = VertexAIManager('YOUR_PROJECT_ID', 'us-central1')
manager.create_custom_endpoint(
    'neuromod-production',
    'gcr.io/YOUR_PROJECT_ID/neuromod-full:latest',
    model_name='meta-llama/Meta-Llama-3.1-8B'
)
"

echo "‚úÖ Stage 3 deployment complete"
```

## üéØ **Deployment Checklist**

### **Pre-Deployment**
- [ ] Run local tests: `python test_prediction_server_local.py`
- [ ] Run container tests: `./test_container.sh`
- [ ] Run risk assessment: `python derisk_vertex_deployment.py`
- [ ] Verify all tests pass
- [ ] Check resource requirements
- [ ] Prepare rollback plan

### **Stage 1 Deployment**
- [ ] Build minimal container
- [ ] Push to GCR
- [ ] Deploy to Vertex AI
- [ ] Test health endpoint
- [ ] Test basic prediction
- [ ] Monitor logs for 15 minutes
- [ ] Verify no crashes

### **Stage 2 Deployment**
- [ ] Build full container
- [ ] Push to GCR
- [ ] Deploy to Vertex AI
- [ ] Test all endpoints
- [ ] Test neuromodulation
- [ ] Test probe system
- [ ] Test emotion tracking
- [ ] Monitor for 30 minutes

### **Stage 3 Deployment**
- [ ] Deploy with production model
- [ ] Test large model loading
- [ ] Test performance
- [ ] Monitor memory usage
- [ ] Load test
- [ ] Monitor for 1 hour
- [ ] Verify stability

## üí° **Pro Tips**

1. **Never skip stages** - each stage catches different issues
2. **Monitor closely** - watch logs and metrics during deployment
3. **Have rollback ready** - know how to revert quickly
4. **Test incrementally** - add features one at a time
5. **Document issues** - learn from each deployment

## üéâ **Success Criteria**

**Deployment is successful when:**
- ‚úÖ All stages complete without critical failures
- ‚úÖ All endpoints respond correctly
- ‚úÖ Performance meets requirements
- ‚úÖ No crashes or errors in logs
- ‚úÖ System is stable for 24+ hours

**If any stage fails:**
- üîß Fix the issue
- üîÑ Retest locally
- üöÄ Redeploy that stage
- üìä Monitor closely
